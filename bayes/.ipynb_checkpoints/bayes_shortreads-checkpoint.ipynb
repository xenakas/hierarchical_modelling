{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Methods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is the Bayesian Inference controversial?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Bayesians claim that the parameters are random so that their credible interval is a valid probability argument. This interpretation looks nice but the credible interval of the parameters of interest not only depends on the likelihood but also the prior, which is usually hard to obtain.\n",
    "\n",
    "2. When the likelihood and prior is complicated, the inference has to rely on the MCMC sampling, which can be really slow in most of the real-world cases.\n",
    "\n",
    "3. The biggest controversy about Bayesian inference is that you must quantify your prior knowledge about the question at hand. This makes it possible to actually influence your results, either accidentally or on purpose.\n",
    "\n",
    "This is a genuine concern, and any Bayesian analysis worth it’s salt must demonstrate that the chosen priors aren’t influencing the final results. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shrinkage\n",
    "\n",
    "Shrinkage is implicit in Bayesian inference and penalized likelihood inference, and explicit in James–Stein-type inference. In contrast, simple types of maximum-likelihood and least-squares estimation procedures do not include shrinkage effects, although they can be used within shrinkage estimation schemes.\n",
    "\n",
    "Shrinkage is implicit in Bayesian inference and penalized likelihood inference, and explicit in James–Stein-type inference. In contrast, simple types of maximum-likelihood and least-squares estimation procedures do not include shrinkage effects, although they can be used within shrinkage estimation schemes.\n",
    "\n",
    "####  [Stein's paradox](http://statweb.stanford.edu/~ckirby/brad/other/Article1977.pdf)\n",
    "\n",
    "\n",
    "> The best guess about the future is usually  obtained by computing the average of past events. Stein's paradox defines corcumstances in which there are estimators better that the arithmetic average\n",
    "\n",
    "\n",
    "\n",
    "Stein's  paradox, in decision theory and estimation theory, is the phenomenon that when three or more parameters are estimated simultaneously, there exist combined estimators more accurate on average (that is, having lower expected mean squared error) than any method that handles the parameters separately. \n",
    "\n",
    "\n",
    "An intuitive explanation is that optimizing for the mean-squared error of a combined estimator is not the same as optimizing for the errors of separate estimators of the individual parameters. In practical terms, if the combined error is in fact of interest, then a combined estimator should be used, even if the underlying parameters are independent. On the other hand, if one is instead interested in estimating an individual parameter, then using a combined estimator does not help and is in fact worse.\n",
    "\n",
    "#### [Implications of Stein’s Paradox for Environmental Standard Compliance Assessment](https://pubs.acs.org/doi/pdf/10.1021/acs.est.5b00656)\n",
    "\n",
    "The implications of Stein’s paradox stirred considerable debate in statistical circles when the concept was first introduced in the 1950s. \n",
    "\n",
    "The paradox arises when we are interested in estimating the means of several variables simultaneously. In this situation, the best estimator for an individual mean, the sample average, is no longer the best. \n",
    "\n",
    "Rather, a shrinkage estimator, which shrinks individual sample averages toward the overall average is shown to have improved overall accuracy. \n",
    "\n",
    "Although controversial at the time, the concept of shrinking toward overall average is now widely accepted as a good practice for improving statistical stability and reducing error, not only in simple estimation problems, but also in complicated modeling problems. \n",
    "\n",
    "In this essay, we introduce Stein’s paradox and its **modern generalization, the Bayesian hierarchical model**. \n",
    "Bayesian hierarchical model can improve overall estimation accuracy, thereby improving our confidence in the assessment results, especially for standard compliance assessment of waters with **small sample sizes.**\n",
    "\n",
    "[Connection between Stein's paradox, ridge regression, and random effects in mixed models](https://stats.stackexchange.com/questions/122062/unified-view-on-shrinkage-what-is-the-relation-if-any-between-steins-paradox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
