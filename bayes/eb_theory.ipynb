{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Empirical Bayes method](https://en.wikipedia.org/wiki/Empirical_Bayes_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empirical Bayes uses the data to set the hyperparameters of the prior. Performing Bayesian inference with this prior then gets you a sort of shrinkage and can be viewed as an approximation to a hierarchical Bayesian model.\n",
    "\n",
    "\n",
    "EB ignores the uncertainty in the hyper-parameters, whereas HBM attempts to include it in the analysis. HMB is a good idea where there is little data and hence significant uncertainty in the hyper-parameters, which must be accounted for. On the other hand for large datasets EB becomes more attractive as it is generally less computationally expensive and the the volume of data often means the results are much less sensitive to the hyper-parameter settings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empirical Bayes methods are procedures for statistical inference in which the prior distribution is estimated from the data. This approach stands in contrast to standard Bayesian methods, for which the prior distribution is fixed before any data are observed. Despite this difference in perspective, empirical Bayes may be viewed as an approximation to a fully Bayesian treatment of a hierarchical model wherein the parameters at the highest level of the hierarchy are set to their most likely values, instead of being integrated out. Empirical Bayes, also known as maximum marginal likelihood, represents one approach for setting hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  [James-Stein estimators](http://www.stats.ox.ac.uk/~reinert/stattheory/chapter1107.pdf)\n",
    "\n",
    "\n",
    "Assume that $θ_i ∼ N (0, τ^2)$, then $p(x|τ^2) = N (0,(1 + τ^2)I_p)$, and\n",
    "the posterior for $θ$ given the data is\n",
    "$$θ|x ∼ N (τ^2/(1 + τ^2) x, 1/(1 + τ)^2I_p)$$\n",
    "\n",
    "Under quadratic loss, the Bayes estimator $δ(x)$ of $θ$ is the posterior\n",
    "mean $τ^2/(1 + τ^2) x$\n",
    "\n",
    "In the empirical Bayes approach, we would use the m.l.e. for \n",
    "$τ^2$\n",
    "and the empirical Bayes estimator is the estimated posterior mean,\n",
    "$$δ^{EB}(x) = \\hat{τ}^2/(1 + \\hat{τ}^2)x = (1 − p/||x||^2)^+ x$$\n",
    "is the truncated James-Stein estimator. It can can be shown to\n",
    "outperform the estimator $δ(x) = x$.\n",
    "\n",
    "Alternatively, the best unbiased estimator of $1/(1 + τ^2)$ is $(p−2)/||x||$\n",
    "giving\n",
    "$$δ^{EB}(x) = (1 − p/|| x ||^2) x$$\n",
    "This is the James-Stein estimator. It can be shown that under\n",
    "quadratic loss function the James-Stein estimator has a risk function\n",
    "that is uniformly better than $δ(x) = x$.\n",
    "\n",
    ">Note: both estimators tend to ”shrink” towards 0. It is now known\n",
    "to be a very general phenomenon that when comparing three or more\n",
    "populations, the sample mean is not the best estimator. ”Shrinkage”\n",
    "estimators are an active area of research\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [ Hierarchical vs Empirical Bayesian methods](http://www.stats.ox.ac.uk/~reinert/stattheory/chapter1107.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical Bayes: $\n",
    "θ|β ∼ π_1(θ|β)$, where $β ∼ π_2(β)$\n",
    "so that then $π(θ) = \\int π_1(θ|β)π_2(β)dβ$.\n",
    "\n",
    "\n",
    "\n",
    "For simulation in hierarchical models: we simulate first from $β$,\n",
    "then, given $β$, we simulate from $θ$. We hope that the distribution of\n",
    "$β$ is easy to simulate, and also that the conditional distribution of $θ$\n",
    "given $β$ is easy to simulate. This approach is particularly useful for\n",
    "MCMC (Markov chain Monte Carlo) methods, e.g.: see next term.\n",
    "\n",
    "\n",
    "Empirical Bayes:\n",
    "$p(x|λ) = \\int f(x|θ)π(θ|λ)dθ$\n",
    "\n",
    "Rather than specifying $λ$, we estimate $λ$ by $\\hat{λ}$, for example by frequentist methods, based on $p(x|λ)$,  $π(θ|x, \\hat{λ})$ \n",
    "is called a pseudo-posterior\n",
    "\n",
    "\n",
    "The empirical Bayes approach\n",
    "- is neither fully Bayesian nor fully frequentist;\n",
    "- depends on $\\hat{λ}$, different $\\hat{λ}$ will lead to different procedures;\n",
    "- if $\\hat{λ}$ is consistent, then asymptotically will lead to coherent Bayesian\n",
    "analysis.\n",
    "- often outperforms classical estimators in empirical terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Empirical and Hierarchical Bayes](https://www.cs.ubc.ca/~schmidtm/Courses/540-W16/L19.pdf)\n",
    "\n",
    "> Why the beta distribution? It’s a flexible distribution that includes uniform as special case (if $α = 1$ and $β = 1$.)\n",
    "\n",
    "- Likelihood $p(x|θ)$. Probability of seeing data given parameters.\n",
    "- Prior $p(θ|α, β)$.\n",
    "Belief that parameters are correct before we’ve seen data.\n",
    "- Posterior $p(θ|x, α, β)$.\n",
    "Probability that parameters are correct after we’ve seen data.\n",
    "- Posterior predictive $p(\\hat{x}|x, α, β)$.\n",
    "Probability of new data given old, integrating over parameters, \n",
    "tells us which prediction is most likely given data and prior.\n",
    "- Marginal likelihood (also called evidence) $p(x|α, β)$.\n",
    "Probability of seeing data given hyper-parameters.\n",
    "\n",
    "Hyper-parameters $α$ and $β$ are like “pseudo-counts” in our mind before we flip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Learning the Prior from Data:\n",
    "\n",
    "_Can we use the data to set the hyper-parameters_\n",
    "\n",
    "- In theory: No!\n",
    "    - It would not be a “prior”. \n",
    "- In practice: Yes!\n",
    "    - Approach 1: use a validation set or cross-validation as before.\n",
    "    - Approach 2: optimize the marginal likelihood,\n",
    "$$ p(y|X, λ) = \\int_w p(y|X, w)p(w|λ)dw$$\n",
    "\n",
    "Also called type II maximum likelihood or evidence maximization or empirical Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Overivew of Bayesian Variable Selection\n",
    "\n",
    "[Empirical Bayes vs. fully Bayes variable selection](http://www-stat.wharton.upenn.edu/~edgeorge/Research_papers/CG%20JSPI%202008.pdf)\n",
    "\n",
    "- If we fix $λ$ and use L1-regularization (Bayesian lasso), posterior is not sparse.\n",
    "Probability that a variable is exactly 0 is zero.\n",
    "L1-regularization only lead to sparsity because the MAP point estimate is sparse.\n",
    "- Type II maximum likelihood leads to sparsity in the posterior because variance\n",
    "goes to zero.\n",
    "    - Weird fact: yields sparse solutions (automatic relevance determination).\n",
    "Can send $λ_j → ∞$, concentrating posterior for $w_j$ at 0.\n",
    "This is L2-regularization, but empirical Bayes naturally encouages sparsity.\n",
    "(Non-convex and theory not well understood, but recent work shows:\n",
    "Never performs worse than L1-regularization, and exists cases where it does better)\n",
    "- We can encourage sparsity in Bayesian models using a spike and slab prior:\n",
    "    - Mixture of Dirac delta function 0 and another prior with non-zero variance. Places non-zero posterior weight at exactly 0. Posterior is still non-sparse, but answers the question “what is the probability that variable is non-zero”?\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bayesian Model Selection and Averaging\n",
    "\n",
    "Bayesian model selection (“type II MAP”): maximize hyper-parameter posterior,\n",
    "which further takes us away from overfitting (thus allowing more complex models).\n",
    "\n",
    "\n",
    "Bayesian model averaging considers posterior over hyper-parameters.\n",
    "We could also maximize marginal likelihood of $γ$, (“type III ML”)\n",
    "\n",
    "- Posterior predictive lets us directly model what we want given hyper-parameters.\n",
    "- Marginal likelihood is probability seeing data given hyper-parameters.\n",
    "- Empirical Bayes optimizes this to set hyper-parameters:\n",
    "    - Allows tuning a large number of hyper-parameters.\n",
    "    - Bayesian Occam’s razor: naturally encourages sparsity and simplicity.\n",
    "- Hierarchical Bayes goes even more Bayesian with prior on hyper-parameters.\n",
    "    - Leads to Bayesian model selection and Bayesian model averaging.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
