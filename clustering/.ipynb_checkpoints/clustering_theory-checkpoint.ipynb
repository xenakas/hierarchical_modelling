{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Clustering of time series data](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.115.6594&rep=rep1&type=pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.cs.columbia.edu/~gravano/Papers/2017/tods17.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.115.6594&rep=rep1&type=pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3984869/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.datanovia.com/en/blog/types-of-clustering-methods-overview-and-quick-start-r-code/#partitioning-clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://datawookie.netlify.com/blog/2017/04/clustering-time-series-data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Clustering of Time Series Subsequences is Meaningless](http://www.cs.ucr.edu/~eamonn/meaningless.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Whole Clustering: The notion of clustering here is similar to that of conventional clustering of discrete objects. Given a set of individual time series data, the objective is to group similar time series into the same cluster.\n",
    "- Subsequence Clustering: Given a single time series, sometimes in the form of streaming time series, individual time series (subsequences) are extracted with a sliding window. Clustering is then performed on the extracted time series subsequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this work we make a surprising claim. Clustering of time series subsequences is meaningless! In particular,\n",
    "clusters extracted from these time series are forced to obey a certain constraints that are pathologically unlikely to be\n",
    "satisfied by any dataset, and because of this, the clusters extracted by any clustering algorithm are essentially\n",
    "random.\n",
    "\n",
    "Since we use the word “meaningless” many times in this paper, we will take the time to define this term. All\n",
    "useful algorithms (with the sole exception of random number generators) produce output that depends on the input.\n",
    "For example, a decision tree learner will yield very different outputs on, say, a credit worthiness domain, a drug\n",
    "classification domain, and a music domain. We call an algorithm “meaningless” if the output is independent of the\n",
    "input. As we prove in this paper, the output of STS clustering does not depend on input, and is therefore\n",
    "meaningless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Background on Clustering\n",
    "\n",
    "Algorithm Hierarchical Clustering\n",
    "1. Calculate the distance between all objects. Store the results in a distance matrix.\n",
    "2. Search through the distance matrix and find the two most similar clusters/objects.\n",
    "3. Join the two clusters/objects to produce a cluster that now has at least 2 objects.\n",
    "4. Update the matrix by calculating the distances between this new cluster and all other clusters.\n",
    "5. Repeat step 2 until all cases are in one cluster.\n",
    "\n",
    "Algorithm k-means\n",
    "1. Decide on a value for k.\n",
    "2. Initialize the k cluster centers (randomly, if necessary).\n",
    "3. Decide the class memberships of the N objects by assigning them to the nearest cluster center.\n",
    "4. Re-estimate the k cluster centers, by assuming the memberships found above are correct.\n",
    "5. If none of the N objects changed membership in the last iteration, exit. Otherwise go to 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use these two numbers to create a fraction:\n",
    "between set X and Y distance\n",
    "within set X distance\n",
    "\n",
    "$$ clustering \\ meaningfulness = \\frac{within \\ set \\ X \\ distance}{between \\ set \\ X \\ and \\ Y  \\ distance} $$\n",
    "\n",
    "We can justify calling this number “clustering meaningfulness” since it clearly measures just that. If, for any dataset,\n",
    "the clustering algorithm finds similar clusters each time regardless of the different initial seeds, the numerator should\n",
    "be close to zero. In contrast, there is no reason why the clusters from two completely different, unrelated datasets\n",
    "should be similar. Therefore, we should expect the denominator to be relatively large. So overall we should expect\n",
    "that the value of clustering meaningfulness  be close to zero when X and Y are sets of cluster centers derived\n",
    "from different datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implications of Theorem 1 become clearer when we consider the following well documented fact. For any\n",
    "dataset, the weighted (by cluster membership) average of k clusters must sum up to the global mean. The implication\n",
    "for STS clustering is profound. Since the global mean for STS clustering is a straight line, then the weighted average\n",
    "of k-clusters must in turn sum to a straight line. However, there is no reason why we should expect this to be true of\n",
    "any dataset, much less every dataset. This hidden constraint limits the utility of STS clustering to a vanishing small\n",
    "set of subspace of all datasets. The out-of-phase sine waves as cluster centers that we get from the last section\n",
    "conforms to this theorem, since their weighted average, as expected, sums to a straight line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An even more tantalizing piece of evidence exists. In the 1920’s “data miners” were excited to find that by\n",
    "preprocessing their data with repeated smoothing, they could discover trading cycles. Their joy was shattered by a\n",
    "theorem by Evgeny Slutsky (1880-1948), who demonstrated that any noisy time series will converge to a sine wave\n",
    "after repeated applications of moving window smoothing (Kendall, 1976). While STS clustering is not exactly the\n",
    "same as repeated moving window smoothing, it is clearly highly related. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm motif-based-clustering\n",
    "1. Decide on a value for k.\n",
    "2. Discover the K-motifs in the data, for K = k × c (c is some constant, in the region of about 2 to 30)\n",
    "3. Run k-means, or k partitional hierarchical clustering, or any other clustering algorithm on the subsequences covered by Kmotifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
