{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Forecast Combinations](http://www.oxford-man.ox.ac.uk/sites/default/files/events/combination_Sofie.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forecast combinations have been successfully applied in several areas of\n",
    "forecasting GNP and estimation of GDP\n",
    "\n",
    "- Why combine?\n",
    "Many models or forecasts with similar predictive accuracy; diversification gains\n",
    "- When to combine?\n",
    "Individual forecasts are misspecified; \n",
    "unstable forecasting environment (past track record unreliable);\n",
    "short track record\n",
    "- What to combine? \n",
    "Forecasts using different information sets;\n",
    "forecasts based on different modeling approaches (linear/nonlinear)\n",
    "\n",
    "- Combinations of forecasts is motivated by \n",
    "misspecified forecasting models due to (e.g., structural breaks); \n",
    "diversification across forecasts\n",
    "\n",
    "\n",
    "__Essentials of forecast combination__:\n",
    "- Dimensionality reduction: Combination reduces the information in a vector of forecasts to a single summary measure using a set of combination weights\n",
    "- Optimal combination chooses weights to minimize the expected loss of the combined forecast \n",
    "    - more accurate forecasts tend to get larger weights, \n",
    "    - combination weights also reáect correlations across forecasts\n",
    "    - estimation error is important to combination weights\n",
    "- Irrelevance Proposition: In a world with no model misspecification, infinite data samples (no estimation error) and complete access to the information sets underlying the individual forecasts, there is no need for forecast combination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combined forecast $f (\\hat{f}_1, \\hat{f}_2) $ dominates individual forecasts if\n",
    "$$ E[L(\\hat{f}_{i,y_{T+h}})] > \\min_{f (.)} E [L(f (\\hat{f}_1, \\hat{f}_2), y_{T +h} ), for \\  i = 1, 2$$\n",
    "where $L$ - loss function (e.g., MSE loss)\n",
    "\n",
    "\n",
    "Forecast combination is essentially a model selection and parameter\n",
    "estimation problem with special constraints on the estimation problem\n",
    "\n",
    "Using a middle step of first constructing forecasts limits the flexibility of the final forecasting model. Why not directly map the underlying data to the forecasts?\n",
    "- Estimation error plays a key role in the risk of any given method. Model combination yields a risk function which, through parsimonious use of the data, could result in an attractive risk function\n",
    "- Combined forecast can be viewed simply as a different estimator of the final model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theory:\n",
    "\n",
    "Specialized concepts in optimal forecast combination arise from additional restrictions placed on the search for combination models\n",
    "Because the underlying data are forecasts, they can be expected to obtain non-negative weights that sum to unity. \n",
    "Such constraints can be used to reduce the relevant parameter space for the combination weights and offer a more attractive risk function\n",
    "\n",
    "Solving for the MSE-optimal combination weights, can lead to \n",
    "negative combination weight.\n",
    "Negative weight on a forecast does not mean that it has no value - it means\n",
    "the forecast can be used to offset the prediction errors of other models\n",
    "\n",
    "Equal weights (EW) play a special role in forecast combination. \n",
    "EW are optimal in population when the individual forecast errors have\n",
    "identical variance and identical pair-wise correlations.  \n",
    "This situation holds to a close approximation when all models are based on similar data and perform roughly the same.\n",
    "More generally, EW are the optimal combination weights when the unit\n",
    "vector lies in the eigen space of $Σ_e$.\n",
    "\n",
    "Simple combination schemes such as EW satisfy these constraints and do not require estimation of any parameters.\n",
    "EW can be viewed as a reasonable prior when no data has been observed\n",
    "\n",
    "Simple combination methods:\n",
    "- Equal-weighted forecast\n",
    "- Median forecast\n",
    "- Trimmed mean. Order forecasts. Trim top/bottom $λ$%\n",
    "\n",
    "\n",
    "\n",
    "Existence of many estimation methods boils down to a number of standard issues\n",
    "in constructing forecasts:\n",
    "- role of estimation error\n",
    "- lack of a single optimal estimation scheme\n",
    "- simple methods are difficult to beat in practice\n",
    "- common baseline is to use a simple EW average of forecasts:\n",
    "- no estimation error here since the combination weights are imposed rather than estimated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empirical studies often find that simple equal-weighted forecast combinations perform very well compared with more sophisticated combination schemes that rely on estimated combination weights\n",
    "[Smith and Wallis (2009)] \n",
    "\n",
    "Errors introduced by estimation of the combination weights could overwhelm any gains from setting the weights to their optimal values over using equal weights\n",
    "\n",
    "Explanations of the puzzle based on estimation error must show that\n",
    "estimation error is large and/or \n",
    "gains from setting the combination weights to their optimal values are small\n",
    "relative to using equal weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bates-Granger restricted least squares\n",
    "\n",
    "Bates and Granger (1969): use plug-in weights in the optimal solution based on the estimated variance-covariance matrix\n",
    "\n",
    "This is numerically identical to restricted least squares estimator of the\n",
    "weights from a regression of the outcome on the vector of forecasts \n",
    "and no intercept subject to the restriction that the coefficients sum to one\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diebold and Pauly (1987) shrinkage estimator\n",
    "\n",
    "Forecast combination weights formed as a weighted average of the prior\n",
    "$ω_p = ι_m/m$ and the least squares estimates $\\hat{ω}_{OLS}$:\n",
    "$$ \\hat{ω}_B = \\tilde{A} \\hat{ω}_OLS + (I-\\tilde{A})ι_m/m$$\n",
    "\n",
    "__Empirical Bayes approach__ sets $\\tilde{A} = I(1 - \\hat{σ}^2/\\hat{τ}^2)$\n",
    "- $\\hat{τ}^2 = (\\hat{ω}_{OLS}  ω_p )' (\\hat{ω}_{OLS} - ω_p )/tr[(Z'_f\n",
    "Z_f)]^{-1} $\n",
    "- $\\hat{\\sigma}^2$ - MLE for variance of the residuals from the OLS combination regression\n",
    "- $Z_f$ - matrix of regressors (ignoring the constant)\n",
    "- $Z'_f Z_f$ is an unscaled estimate of the variance-covariance matrix of the forecasts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aiolfi and Timmermann (2006) robust weighting scheme\n",
    "\n",
    "Weights forecast models inversely to their rank, $Rank_{it+h|t}$\n",
    "$$\\hat{ω}_{it+h|t} = \\frac{Rank^{-1}_{it+h|t}}{\\sum^m_{i=1} Rank^{-1}_{it+h|t}}$$\n",
    "Best model gets a rank of 1, second best model a rank of 2,etc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bates and Granger (1969) Adaptive combination weights\n",
    "\n",
    "Adaptive estimation schemes: \n",
    "rolling window of the forecast models' relative performance over the most\n",
    "recent observations\n",
    "\n",
    "Adaptive updating scheme discounts older performance, $λ \\in (0;1)$\n",
    "The closer to unity is $λ$, the smoother the combination weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time-varying combination weights\n",
    "\n",
    "- Time-varying parameter (Kalman filter)\n",
    "- Discrete (observed) state switching (Deutsch et al., 1994)\n",
    "- Regime switching weights (Elliott and Timmermann, 2005)\n",
    "\n",
    "Forecast combinations can work well empirically because they provide\n",
    "insurance against model instability\n",
    "- Empirically, Elliott and Timmermann (2005) allow for regime switching in combinations of forecasts  and find strong evidence that the relative performance of the underlying forecasts changes over time\n",
    "- Performance of combined forecasts tends to be more stable than that of individual forecasts used in the empirical combination study of Stock and Watson (2004)\n",
    "- Combination methods that attempt to explicitly model time-variations in the combination weights often fail to perform well, suggesting that regime switching or model breakdown can be difficult to predict or even to track through time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayesian Model Averaging\n",
    "\n",
    "When the data underlying the individual forecasts is observed, we can\n",
    "construct forecasts from many different models and average over the\n",
    "resulting forecasts\n",
    "\n",
    "Same issues as when only the forecasts are observed - but new possibilities\n",
    "like BMA (Bayesian Model Averaging) arise\n",
    "\n",
    "But we do not directly observe the outcome density we only observe a\n",
    "draw from this and so cannot directly choose the weights to minimize the loss between this object and the combined density\n",
    "\n",
    "Kullback Leibler (KL) loss for a linear combination of densities $\\sum^m_{i=1} ω_i p_{it}(y )$\n",
    "relative to some unknown true density $p(y )$ is given by\n",
    "$$KL = \\int p(y )\\ln{(p(y ))}dy -   \\int p(y )\\ln{(\\sum^m_{i=1} ω_i p_i(y ))} dy = C -  E \\ln{(\\sum^m_{i=1} ω_i p_i(y ))}$$\n",
    "where $C$ is constant for all choices of the weights $ω_i$\n",
    "\n",
    "__Minimizing the KL distance is the same as maximizing the log score in\n",
    "expectation__\n",
    "\n",
    "\n",
    "_Bayesian Model Averaging (BMA):_\n",
    "\n",
    "$$p^c(y) = \\sum^m_{i=1} ω_i p(y |M_i)$$\n",
    "where $M_1, ...., M_m$ - models\n",
    "\n",
    "- BMA weights predictive densities by the posteriors of the models, $M_i$\n",
    "- BMA is a model averaging procedure rather than a predictive density combination procedure per se\n",
    "- BMA assumes the availability of both the data underlying each of the densities, $p_i(y ) = p(y |M_i)$, and knowledge of how that data is employed to obtain a predictive density\n",
    "\n",
    "Computation of $p(M_i|Z)$ requires computation of the marginal likelihood $p(Z|Mi)$ which can be time consuming\n",
    "\n",
    "If the models' marginal likelihoods are difficult to compute, one can use a simple approximation based on BIC\n",
    "\n",
    "\n",
    "Madigan and Raftery (1994) suggest removing models for which $p(M_i|Z) $ is much smaller than the posterior probability of the best model\n",
    "\n",
    "\n",
    "BMA forecasts are more robust than individual forecasts, with unbiased and serially uncorrelated forecast errors. Model uncertainty reduces the strength of the evidence on return predictability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Averaging and the Optimal Combination of Forecasts](https://econweb.ucsd.edu/~gelliott/AveragingOptimal.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal combination of forecasts, detailed in Bates and Granger (1969), has empirically often been overshadowed in practice by using the simple average instead.\n",
    "\n",
    "Explanations of why averaging might in practice work better than constructing the optimal combination have centered on estimation error and the effects variations of the data generating process have on this error. \n",
    "\n",
    "The flip side of this explanation is that\n",
    "the size of the gains must be small enough to be outweighed by the estimation error.\n",
    "This paper examines the sizes of the theoretical gains to optimal combination, providing\n",
    "bounds for the gains for restricted parameter spaces and also conditions under which\n",
    "averaging and optimal combination are equivalent. The paper also suggests a new\n",
    "method for selecting between models that appears to work well with SPF data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Forecast Combination Puzzle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [A Machine Learning Approach to the Forecast Combination Puzzle](https://halshs.archives-ouvertes.fr/halshs-01317974/document)\n",
    "\n",
    "\n",
    "This paper introduces the only algorithm that automatically manages the\n",
    "forecast combination puzzle. The proposed algorithm adapts the structure of\n",
    "the AB-Prod algorithm introduced in to solve the novel problem of automatically managing the forecast combination puzzle within macroeconomic\n",
    "forecasting. The result is the first distribution-free performance guarantees for\n",
    "both the mean combination and any alternative combination.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Forecast combination algorithms provide a robust solution to noisy data and shifting process dynamics. However in practice, sophisticated combination methods often fail to consistently outperform the simple mean combination.\n",
    "This “forecast combination puzzle” limits the adoption of alternative combination approaches and forecasting algorithms by policy-makers. \n",
    "\n",
    "Through\n",
    "an adaptive machine learning algorithm designed for streaming data, this paper proposes a novel time-varying forecast combination approach that retains distribution-free guarantees in performance while automatically adapting combinations according to the performance of any selected combination approach\n",
    "or forecaster. \n",
    "\n",
    "In particular, the proposed algorithm offers policy-makers the\n",
    "ability to compute the worst-case loss with respect to the mean combination\n",
    "ex-ante, while also guaranteeing that the combination performance is never worse than this explicit guarantee. Theoretical bounds are reported with respect to the relative mean squared forecast error. Out-of-sample empirical performance is evaluated on the seven-country dataset\n",
    "\n",
    "\n",
    "\n",
    "Forecast combination methods often outperform forecasting approaches\n",
    "that estimate parameters on noisy data, structural breaks, inconsistent predictors and changing environmental dynamics. Unfortunately forecast\n",
    "combination methods often fail to consistently outperform the mean combination over varying pools of forecasters and varying horizons. This paper offers\n",
    "the first automatic procedure to manage this so-called “forecast combination\n",
    "puzzle”. Accordingly, a large body of research has focused on the theoretical and empirical development of complex forecast combination procedures\n",
    "that aim to fully exploit the information content within a pool of forecasters.\n",
    "__However, empirical results in the literature demonstrate that existing forecast\n",
    "combination approaches fail to consistently outperform the mean__. This negative\n",
    "result is often referred to as the mean forecast combination puzzle.\n",
    "\n",
    "Building on recent advances in the machine learning literature, this paper introduces the only automatic procedure to manage this puzzle.\n",
    "First, we recast the forecast combination setting as a game of “prediction\n",
    "with expert advice”. Next, we adapt the general structure of the AB-Prod algorithm to automatically hedge performance against the mean\n",
    "combination, inheriting its distribution-free performance guarantees \n",
    "\n",
    "##### AB-Prod is computed as follows:\n",
    "\n",
    "Input:\n",
    "- Combination algorithms A and B\n",
    "- A history of observations for the target variable, in our case output or inflation, up to the current time t, of length T.\n",
    "- Preference weight $λ_B ∈ (0, 1)$ for the algorithm $B$.\n",
    "\n",
    "Initialization:\n",
    "- $λ_{A,0} = 1 − λ_B$\n",
    "- Learning rate $η = \\min (\\sqrt{\\frac{-\\log(1-λ_B)}{T}},1/2)$\n",
    "- Set $s_0$ \n",
    "\n",
    "Repeat the following for each observation from time $t = 0, . . . , T$:\n",
    "- Compute combination weight $s_t = \\frac{λ_{A,t}}{λ_{A,t} + λ_B}$\n",
    "- Observe the target variable $y_t$ and compute the loss $l_{A,t}$ and $l_{B,t}$.\n",
    "- Compute the combination loss $l_{AB−Prod,t} = s_tl_{A,t} + (1 − s_t)l_{B,t}$.\n",
    "- Compute the deviation $δ_t = l_{B,t} − l_{A,t}$\n",
    "- Update the Score $λ_{A,t+1} = λ_{A,t}(1 + ηδ_t)$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Learning Time-Varying Forecast Combinations](https://simpolproject.eu/download/dolfins_research/mandel2016learning.pdf)\n",
    "\n",
    "Combining forecasts has been demonstrated as a robust solution to noisy\n",
    "data, structural breaks, unstable forecasters and shifting environmental dynamics. In practice, sophisticated combination methods have failed to consistently outperform the mean over multiple horizons, pools of varying forecasters\n",
    "and different endogenous variables. This paper addresses the challenge to “develop methods better geared to the intermittent and evolving nature of predictive relations”, noted in Stock and Watson (2001), by proposing an adaptive\n",
    "nonparametric “meta” approach that provides a time-varying hedge against\n",
    "the performance of the mean for any selected forecast combination approach.\n",
    "This approach arguably solves the so-called “Forecast Combination Puzzle”\n",
    "using a meta-algorithm that adaptively hedges weights between the mean and\n",
    "a specific forecast combination algorithm or pool of forecasters augmented\n",
    "with one or more forecast combination algorithms. Theoretical performance\n",
    "bounds are reported and empirical performance is evaluated on the sevencountry macroeconomic output and inflation dataset introduced in Stock and\n",
    "Watson (2001) as well as the Euro-area Survey of Professional Forecasters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [A Simple Explanation of the Forecast Combination Puzzle](https://warwick.ac.uk/fac/soc/economics/staff/academic/wallis/publications/smithwallis_obes_09.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__This article presents a formal explanation of the forecast combination puzzle, that simple combinations of point forecasts are repeatedly found to outperform sophisticated weighted combinations in empirical applications.__ The explanation lies in the\n",
    "effect of finite-sample error in estimating the combining weights. A small Monte Carlo study and a reappraisal of an empirical study by Stock and Watson support this explanation. The Monte Carlo evidence, together with a large-sample\n",
    "approximation to the variance of the combining weight, also supports the popular\n",
    "recommendation to ignore forecast error covariances in estimating the weight.\n",
    "\n",
    "\n",
    "Three main conclusions emerge from the foregoing analysis.\n",
    "- If the optimal combining weights are equal or close to equality, a simple average of competing forecasts is expected to be more accurate, in terms of MSFE, than a combination based on estimated weights. The parameter estimation effect is not large, nevertheless it explains the forecast combination puzzle.\n",
    "- However, if estimated weights are to be used, then it is better to neglect any covariances between forecast errors and base the estimates on inverse MSFEs alone, than to use the optimal formula originally given by Bates and Granger for two forecasts, or its regression generalization for many forecasts. \n",
    "- When the number of competing forecasts is large, so that under equal weighting each has a very small weight, the simple average can gain in efficiency by trading off a small bias against a larger estimation variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Some Theoretical Results on Forecast Combinations](http://paneldataconference2015.ceu.hu/Program/Felix-Chan.pdf)\n",
    "\n",
    "\n",
    "By setting up the forecast combination problem as a panel data model, the paper was\n",
    "able to provide the necessary and sufficient condition for optimal weight as well as the\n",
    "necessary and sufficient condition for simple average to be the optimal weight under\n",
    "Mean Squared Forecast Errors (MSFE). It also provided theoretical justifications on\n",
    "the superior forecast performance of simple average or individual models in the MSFE\n",
    "sense. The paper also provided a theoretical exposition on the relative performance of\n",
    "simple average and estimated optimal weight.\n",
    "\n",
    "The results show that the performance of simple average can often outperform the estimated optimal weight in the presence of\n",
    "estimation error. This theoretical justification is consistent with the empirical observation that the simple average often has superior performance over estimated optimal\n",
    "weight\n",
    "\n",
    "The paper also investigated the forecast combination problem under Mean Absolute\n",
    "Deviation (MAD). By applying the __Fundamental Theorem of Linear Programming,__ the\n",
    "paper was able to establish the necessary and sufficient condition for the simple average\n",
    "to outperform a single model in the MAD sense. This result is new and the method\n",
    "adopted in the paper might suggest a feasible way to analyse the forecast combination\n",
    "problems for non-differentiable forecast criteria further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [On the Forecast Combination Puzzle](https://arxiv.org/pdf/1505.00475.pdf)\n",
    "\n",
    "It is often reported in forecast combination literature that a simple average\n",
    "of candidate forecasts is more robust than sophisticated combining methods. This phenomenon is usually referred to as the “forecast combination puzzle”. Motivated by this\n",
    "puzzle, we explore its possible explanations including estimation error, invalid weighting\n",
    "formulas and model screening. We show that existing understanding of the puzzle should\n",
    "be complemented by the distinction of different forecast combination scenarios known as\n",
    "combining for adaptation and combining for improvement. Applying combining methods without consideration of the underlying scenario can itself cause the puzzle. Based\n",
    "on our new understandings, both simulations and real data evaluations are conducted\n",
    "to illustrate the causes of the puzzle. We further propose __a multi-level AFTER strategy__\n",
    "that can integrate the strengths of different combining methods and adapt intelligently\n",
    "to the underlying scenario. In particular, by treating the simple average as a candidate\n",
    "forecast, the proposed strategy is shown to avoid the heavy cost of estimation error and,\n",
    "to a large extent, solve the forecast combination puzzle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Is there an optimal forecast combination?](https://dornsife.usc.edu/assets/sites/462/docs/papers/forthcoming/Hsiao_and_Wan_FC08312011.pdf)\n",
    "\n",
    "We consider several geometric approaches for combining forecasts in large samples  a simple\n",
    "eigenvector approach, a mean corrected eigenvector and trimmed eigenvector approach. We give\n",
    "conditions where geometric approach yields identical result as the regression approach. We also\n",
    "consider a mean and a mean and scale corrected simple average of all predictive models for finite\n",
    "sample and give conditions where simple average is an optimal combination. Monte Carlos\n",
    "are conducted to compare the finite sample performance of these and some popular forecast\n",
    "combination and information combination methods and to shed light on the issues of \"forecast\n",
    "combination\" vs \"information combination\". We also try to shed light on whether there exists\n",
    "an optimal forecast combination method by comparing various forecast combination methods\n",
    "to predict __US real output growth rate__\n",
    "and excess equity premium.\n",
    "\n",
    "- We note first that there are periods where the predictions based on fixed and continuously updating forecasts are way off from the actual, but the rolling window approach appears able to narrow the gap. In general, the rolling framework performs better than the other 2 frameworks (except the three GR approaches). \n",
    "- Second, because the information for generating predictive model is readily available, the __rolling window model selection approach__ of selecting the best predictive model appears to yield most accurate predictions. \n",
    "- Third, if information is not readily available, then the ranking of forecast combination methods in the rolling window framework appear to be consistent with the simulation results in which the eigenvector approach of obtaining relative weights for forecasting models yield more accurate predictions than regression approach or Bayesian averaging. However, the mean corrected eigenvector approach appears to dominate simple eigenvector approach, perhaps because some predictive models are biased. \n",
    "- Fourth, perhaps because of frequent \"breaks\" between the actual and predictive models, trimming does not lead to the improvement and the mean corrected simple average yields as good (or a slightly better) forecasts as the mean corrected eigenvector approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
