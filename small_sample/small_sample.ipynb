{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small Sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Best Predictive Small Area Estimation:](https://www.jstor.org/stable/41416406?seq=1#page_scan_tab_contents) [slides](https://ec.europa.eu/eurostat/cros/system/files/9A01_Keynote_Rao-v2_0.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Growing demand for reliable small area statistics but\n",
    "sample sizes are too small to provide direct (or area\n",
    "specific) estimators with acceptable accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Methods for indirect small area estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Traditional post-stratified synthetic estimator: population post-stratified into subgroups $g$ that cut across small areas $i$ and cell population counts $N_{ig}$ are known. Reliable direct estimates $\\hat{Y}_{+g}$ and $ \\hat{N}_{+g}$ of the post-strata totals $Y_{+g}$ and $ N_{+g}$  are used to get a post-stratified synthetic estimator of small area total $Y_i$ as\n",
    "$$\\hat{Y}_i^{PS} = \\sum_g  N_{ig} \\frac{\\hat{Y}_{+g}}{\\hat{N}_{+g}} $$ \n",
    "- Implicit assumption: $\\bar{Y}_{ig} = \\bar{Y}_{+g}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Area-level Fay-Herriot model\n",
    "Direct area estimates and area-level auxiliary data available. Consider simple random sampling within areas $(i =1,...,m)$ and sample area\n",
    "mean $\\bar{y}_i$ as direct estimator of the population area mean\n",
    "$\\bar{Y}_i$. Some areas of interest may not be sampled at all.\n",
    "\n",
    "Sampling model: $\\bar{y}_i = \\bar{Y}_i +  e_i, e_i \\sim  N(0, \\psi_i), \\psi_i  $  - known\n",
    "\n",
    "Linking model: $\\bar{Y}_i = {z}_i' +  v_i, v_i \\sim  N(0, \\sigma_v^2) $ \n",
    "\n",
    "$z_i$ - vector of area-level covariates obtained from census\n",
    "and administrative sources\n",
    "\n",
    "Combined model: $\\bar{y}_i = {z}_i'\\beta +  v_i + e_i $ \n",
    " \n",
    "Special case of a linear mixed model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimal estimation of $\\bar{Y}_i$\n",
    "\n",
    "Best (Bayes) estimator under the assumed model is the\n",
    "conditional expectation of $\\bar{Y}_i$ given the survey estimator\n",
    "$\\bar{y}_i$ and model parameters $β$ and $ σ_v^2$  : \n",
    "\n",
    "$$ \\hat{\\bar{Y}}_iˆB =  γ_i \\bar{y}_i + ( 1 − γ_i) z_i' β$$ \n",
    "$γ_i = \\frac{\\sigma_v^2}{\\sigma_v^2 + \\psi_i} $\n",
    "\n",
    "Best estimator is a weighted combination of direct\n",
    "estimator $\\bar{y}_i$ and __regression synthetic estimator__ $z_i' β$.\n",
    "More weight is given to direct estimator when the\n",
    "sampling variance is small relative to total variance and\n",
    "more weight to the synthetic estimator when sampling\n",
    "variance is large or model variance is small.\n",
    "\n",
    "In practice, we need to estimate model parameters: Fay\n",
    "and Herriot (1979) method of moments (MM),\n",
    "maximum likelihood (ML) or __restricted ML (REML)__. \n",
    "\n",
    "Resulting estimator is called empirical best or __empirical\n",
    "Bayes (EB)__:\n",
    "\n",
    "\n",
    "$$ \\hat{\\bar{Y}}_iˆ{EB} =  \\hat{γ}_i \\bar{y}_i + ( 1 − \\hat{γ}_i) z_i' \\hat{β}$$ \n",
    "\n",
    "\n",
    "- MM does not require normality assumption and the resulting EB estimator is also EBLUP (empirical best linear unbiased prediction) estimator.\n",
    "\n",
    "Customary weighted least squares estimator of $β$ may\n",
    "not be the best from a prediction point of view unless\n",
    "the mean model $z_i'β$  is correctly specified. _Jiang,\n",
    "Nguyen and J. S. Rao (2011)_ proposed alternative \n",
    "estimator of $β$ that makes the EB estimator perform\n",
    "well under misspecification of the mean model.\n",
    "\n",
    "__Preliminary test estimation (Datta et al. 2010):__\n",
    "\n",
    "Test for the hypothesis : $H_0: σ_v^2$, using significance level\n",
    "$α = 0.2$. If $H_0$ is not rejected, use regression synthetic \n",
    "estimator under the model without the random effects,\n",
    "otherwise use the EB estimator.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mean squared prediction error\n",
    "\n",
    "$$ MSPE ( \\hat{\\bar{Y}}_i^{EB}) = E (\\hat{\\bar{Y}}_i^{EB} − \\bar{Y}_i )^2$$  is used as a measure of precision of the EB estimator. If the number of areas $m$  is moderately large then\n",
    "\n",
    "$$ MSPE ( \\hat{\\bar{Y}}_i^{EB}) \\approx g_{1i}(σ_v^2) + g_{2i}(σ_v^2) + g_{3i}(σ_v^2) $$\n",
    "\n",
    "- Leading term $ g_{1i}(σ_v^2) =  γ_i ψ_i$  shows large reduction in MSPE can be achieved relative to the direct estimator $\\bar{y}_i$ if  $γ_i$ is small. \n",
    "- Second term is due to the estimation of $β$ \n",
    "- Last term is due to estimation of $σ_v^2$ and they are of lower order.\n",
    "\n",
    "Alternative methods use resampling including jackknife and bootstrap which are more computer intensive but more widely applicable. MSPE\n",
    "estimation is an area of active research.\n",
    "For the preliminary test estimator, use the MSPE\n",
    "estimator under the model without random effects \n",
    "when $H_0$ is not rejected, otherwise use the usual\n",
    "MSPE estimator. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model selection and checking\n",
    "- Variable selection: Fence method (Jiang et al. 2008), Conditional Akaike Information Criterion (AIC) for  predictive performance (Han 2011)\n",
    "- Model checking: Residual analysis (weighted Q-Q plots, influential diagnostics, Rao 2003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What can go wrong?\n",
    "- Outliers in random effects $v_i$ and  unit errors $e_{ij}$: use robust EBLUP assuming mean zero random effects and unit errors (Sinha and Rao 2009).\n",
    "- Relax the mean assumption by replacing mean function $x'_{ij}β$ in the model by some smooth function and  approximate it by a P-spline. Resulting model has a linear mixed model form so use EBLUP (Opsomer et al. 2008). \n",
    "- Wrong specified model: Use a model assisted approach by treating the model as a working model and then doing design-based bias correction (Lehtonen and Veijanen 1999). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Current Research\n",
    "- Relax normality assumption by assuming a family of skew normal distributions for the random effects and the errors in the nested error model.\n",
    "- WB is also studying EB method without normality assumption, using a mixture of normal distributions approach. \n",
    "- Hierarchical Bayes estimation (Molina, Nandram and Rao 2013): Computationally simpler than the EB as it avoids bootstrap MSE estimation. Permits interval estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [The M3-Competition: results, conclusions and implications](https://pdfs.semanticscholar.org/8461/b79f9747a0caee85522c49bd4655c64e10fb.pdf)\n",
    "\n",
    "Comparison of forecasting methods (TS: 14 obs for yearly data, 16 for quarterly,  forecast each series with ETS (ARIMA)\n",
    "models)\n",
    "\n",
    "- `mcomp`: [rcran](https://cran.r-project.org/web/packages/Mcomp/index.html), [gitpage](http://pkg.robjhyndman.com/Mcomp/)\n",
    "- [The accuracy of extrapolation (time series) methods: Results of a forecasting competition](https://www.researchgate.net/profile/Spyros_Makridakis/publication/245162829_Accuracy_of_Forecasting_An_Empirical_Investigation/links/546641ef0cf25b85d17f5e43/Accuracy-of-Forecasting-An-Empirical-Investigation.pdf)\n",
    "- [The Mcomp Package Example](https://datascienceplus.com/the-mcomp-package/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the M-Competition were similar to those of the earlier Makridakis and Hibon:\n",
    "- Statistically sophisticated or complex methods do not necessarily provide more accurate forecasts than simpler ones.\n",
    "- The relative ranking of the performance   of the various methods varies according to the   accuracy measure being used.\n",
    "- The accuracy when various methods are   being combined outperforms, on average, the  individual methods being combined and does  very well in comparison to other methods.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Methods included in the M3-Competition classified into six categories\n",
    "\n",
    "Naive/simple\n",
    "\n",
    "1. Deseasonalized Naive (Random Walk) \n",
    "2. Single Exponential Smoothing\n",
    "\n",
    "Explicit trend models\n",
    "\n",
    "3. Holt’s Linear Exponential Smoothing (two parameter model)\n",
    "4. Robust-Trend: non-parametric version of Holt’s linear model with median based estimate of trend\n",
    "5. Holt–Winter’s linear and seasonal exponential smoothing (two or three parameter model)\n",
    "6. Dampen Trend Exponential Smoothing and PP-autocast \n",
    "7. Theta-sm: smoothing plus a set of rules for dampening the trend\n",
    "\n",
    "Decomposition\n",
    "\n",
    "8. Theta: Specific decomposition technique, projection and combination of the individual components\n",
    "\n",
    "ARIMA/ARARMA model\n",
    "\n",
    "9. Robust ARIMA univariate Box–Jenkins with/without Intervention Detection \n",
    "10. ARARMA:  Automated Parzen’s methodology with Autoregressive filter\n",
    "\n",
    "Expert system \n",
    "11. ForecastPro: Selects from several methods: Exponential Smoothing/Box Jenkins/Poisson and negative binomial models/Croston’s Method/Simple Moving Average \n",
    "12. RBF: Rule-based forecasting using three methods (random walk, linear regression and Holt’s) to estimate level and trend, involving corrections simplification, automatic feature identification and re-calibration\n",
    "\n",
    "Neural networks\n",
    "\n",
    "13. Automat ANN: Automated Artificial Neural Networks for forecasting purposes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Methods which give the best results: \n",
    "\n",
    "- Theta (__yearly:__ industry; __quarterly:__ micro, macro, finance, demographic, __monthly__: micro)\n",
    "- Robust-Trend (__yearly:__ macro, micro, industry; __monthly:__ macro)\n",
    "- BJ (__yearly:__ finance)\n",
    "- ARARMA (__monthly__: macro)\n",
    "\n",
    "- Automat ANN (__monthly__:finance (2nd), micro (4th))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Problems with small sample sizes](https://garstats.wordpress.com/2017/02/04/small-sample-sizes/amp/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This important aspect is illustrated in the figure below. Columns show distributions that differ in four different ways. The rows illustrate samples of different sizes. The scatterplots were jittered using `ggforce::geom_sina` in R. The vertical black bars indicate the mean of each sample. In row 1, examples 1, 3 and 4 have exactly the same mean. In example 2 the means of the two distributions differ by 2 arbitrary units. The remaining rows illustrate random subsamples of data from row 1. Above each plot, the t value, mean difference and its confidence interval are reported. Even with 100 observations we might struggle to approximate the shape of the parent population. Without additional information, it can be difficult to determine if an observation is an outlier, particularly for skewed distributions. In column 4, samples with n = 20 and n = 5 are very misleading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://garstats.files.wordpress.com/2017/02/figure1.png?w=625)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small sample size could be less of a problem in a Bayesian framework, in which information from prior experiments can be incorporated in the analyses. In the blind and significance obsessed frequentist world, small $n$ is a recipe for disaster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Fitting models to short time series](https://robjhyndman.com/hyndsight/short-time-series/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using least squares estimation, or some other non-regularized estimation method, it is possible to estimate a model only if you have more observations than parameters.  (If you use the LASSO, or some other regularization technique, it is possible to estimate a model with fewer observations than parameters.) However, there is no guarantee that a fitted model will be any good for forecasting, especially when the data are noisy.\n",
    "\n",
    "\n",
    "\n",
    "The only reasonable approach is to first check that there are enough observations to estimate the model, and then to test if the model performs well out-of-sample. With short series, there is not enough data to allow some observations to be witheld for testing purposes. However, the AIC can be used as a [proxy for the one-step forecast out-of-sample MSE](https://robjhyndman.com/hyndsight/aic/). The AIC allows both the number of parameters and the amount of noise to be taken into account.\n",
    "\n",
    "\n",
    "What tends to happen with short series is that the AIC suggests very simple models because anything with more than one or two parameters will produce poor forecasts due to the estimation error.  After applying the auto.arima() function from the forecast package in R to all the series from the M-competition, 32 of 144 series had models with zero parameters (random walks), 95 had models with one parameter.\n",
    "\n",
    "Seasonal models bring their own difficulties because the seasonality usually takes up m-1 \n",
    " degrees of freedom where \n",
    "m\n",
    " is the seasonal period. Fourier terms are one way to reduce the problem — useful whenever the ratio of \n",
    "m\n",
    " to sample size is large. \n",
    " \n",
    " Consequently, at least $p+q+P+Q+d+mD+1$ observations\n",
    "are required to estimate a seasonal ARIMA model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [What to do When Your Sample Size is Not Big Enough](http://www.statisticssolutions.com/what-to-do-when-your-sample-size-is-not-big-enough/)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Alpha Level (significance level): the standard cutoff is .05 for most social science research. An alpha of .01 means there is just a 1% chance that a significant result is a false positive (low chance of Type I error). However, you could consider increasing alpha if you did not achieve an adequate sample size.\n",
    "\n",
    "- More complicated analyses require a higher sample size if you are to achieve adequate power for your study. Remove non-essential variables, and your power should increase. You may also consider using a nonparametric test. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is the minimum number of observations required to perform a statistically significant panel analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, Ferron et al. (2009) showed that a Kenward-Roger correction can provide appropriate standard errors well into the single digits for a fairly simple model and Browne and Draper (2006) were able to obtain unbiased variance components with REML estimation with only 6 units at the highest level (again for a simple model). [Gelman (2006)](http://www.stat.columbia.edu/~gelman/research/published/taumain.pdf) showed that Bayesian methods can produce unbiased estimates of variance components with as few as 3 units at the highest level using a carefully considered, weakly informative prior. If you are worried that 11 is not enough at the highest level, keeping the model as simple as possible is the best way to guard against possible small sample bias (beside collecting more data which is usually not a reasonable suggestion).\n",
    "\n",
    "> Restricted (or residual, or reduced) maximum likelihood (REML) approach is a particular form of maximum likelihood estimation that does not base estimates on a maximum likelihood fit of all the information, but instead uses a likelihood function calculated from a transformed set of data, so that nuisance parameters have no effect.\n",
    "In the case of variance component estimation, the original data set is replaced by a set of contrasts calculated from the data, and the likelihood function is calculated from the probability distribution of these contrasts, according to the model for the complete data set. In particular, REML is used as a method for fitting linear mixed models. In contrast to the earlier maximum likelihood estimation, REML can produce unbiased estimates of variance and covariance parameters.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Bias Reduction of Autoregressive Estimates in Time Series Regression Model through Restricted Maximum Likelihood](https://www.jstor.org/stable/2669758?seq=1#page_scan_tab_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Best method for short time-series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the robustness of different methods to these simple ones, e.g., by not only assessing average accuracy out-of-sample, but also the error variance, using your favorite error measure.\n",
    "\n",
    "The first approach is to use standard/linear time series models (AR, MA, ARMA, etc.), but to pay attention to certain parameters, as described in this post [1] by Rob Hyndman, who does not need an introduction in time series and forecasting world. The second approach, referred to by most of the related literature that I have seen, suggest using non-linear time series models, in particular, the threshold models [2], which include threshold autoregressive model (TAR), self-exiting TAR (SETAR), threshold autoregressive moving average model (TARMA), and TARMAX model, which extends TAR model to exogenous time series. Excellent overviews of the non-linear time series models, including threshold models, can be found in this paper [3] and this paper [4].\n",
    "\n",
    "Stationarity can be a bit tricky when dealing with Bayesian time series models. One choice is to enforce constraints on parameters. Or, you could not. This is fine if you just want to look at the distribution of the parameters. However, if you want to generate the posterior predictive, then you might have a lot of forecasts that explode.\n",
    "\n",
    "The Stan documentation provides a few examples where they put constraints on the parameters of time series models to ensure stationarity. This is possible for the relatively simple models they use, but it can be pretty much impossible in more complicated time series models. If you really wanted to enforce stationarity, you could use a Metropolis-Hastings algorithm and throw out any coefficients that are improper. However, this requires a lot of eigenvalues to be calculated, which will slow things down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ANDERSON-DARLING TEST\n",
    "\n",
    "According to the Anderson-Darling, the minimum sample size is n >5 or at least 6 elements. The test is used to determine the characteristic of the data distribution. In case where the population mean and variance is unknown, the critical value is 0.787 for 0.95 confidence interval. \n",
    "\n",
    "> The Anderson–Darling test is a statistical test of whether a given sample of data is drawn from a given probability distribution. In its basic form, the test assumes that there are no parameters to be estimated in the distribution being tested, in which case the test and its set of critical values is distribution-free. However, the test is most often used in contexts where a family of distributions is being tested, in which case the parameters of that family need to be estimated and account must be taken of this in adjusting either the test-statistic or its critical values. When applied to testing whether a normal distribution adequately describes a set of data, it is one of the most powerful statistical tools for detecting most departures from normality. K-sample Anderson–Darling tests are available for testing whether several collections of observations can be modelled as coming from a single population, where the distribution function does not have to be specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Regression and time series model selection in small samples](https://www.stat.berkeley.edu/~binyu/summer08/Hurvich.AICc.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bias correction to the Akaike information criterion, $AIC$, is derived for regression\n",
    "and autoregressive time series models. The correction is of particular use when the sample\n",
    "size is small, or when the number of fitted parameters is a moderate to large fraction of\n",
    "the sample size. The corrected method, called $AIC_C$, is asymptotically efficient if the true\n",
    "model is infinite dimensional. Furthermore, when the true model is of finite dimension,\n",
    "$AIC_C$ is found to provide better model order choices than any other asymptotically efficient\n",
    "method. Applications to nonstationary autoregressive and mixed autoregressive moving\n",
    "average time series models are also discussed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of AIC, the cause of the overfitting problem becomes evident when one\n",
    "examines plots of AIC and the actual Kullback-Leibler information for the various\n",
    "candidate models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As m, the dimension of the candidate model, increases in comparison to n, the sample size, AIC becomes a strongly negatively biased estimate of the information.\n",
    "This bias can lead to overfitting, even if a maximum cut-off is imposed. The bias of AIC\n",
    "may be attributed to the progressive deterioration, as m/n is increased, in the accuracy\n",
    "of certain Taylor series expansions for the information used in the derivation of AIC.\n",
    "\n",
    "In this paper, we will obtain a bias-corrected version of AIC for nonlinear regression\n",
    "and autoregressive time series models. We achieve this by extending the applicability of\n",
    "the corrected AIC method; AICC is asymptotically efficient, in both regression and time series. For\n",
    "linear regression, AICC is exactly unbiased, assuming that the candidate family of models\n",
    "includes the true model. For nonlinear regression and time series models, the unbiasedness\n",
    "of AICC is only approximate, since the motivation for AICC in these cases is based on\n",
    "asymptotic theory. In all cases, the reduction in bias is achieved without any increase in\n",
    "variance, since AICC may be written as the sum of AIC and a nonstochastic term.  \n",
    "\n",
    "Furthermore,\n",
    "a maximum model order cut-off is not needed for AICC. Among the efficient methods\n",
    "studied AICC is found to perform best. For small samples, AICC is able to out-perform\n",
    "even the consistent methods. In view of the theoretical and simulation results, we argiie\n",
    "that AICC should be used routinely in place of AIC for regression and autoregressive\n",
    "model selection. In addition, we present simulation results demonstrating the effectiveness\n",
    "of AICC for selection of nonstationary autoregressive and mixed autoregressive-moving\n",
    "average time series models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AICC is the sum of AIC and an additional nonstochastic penalty term\n",
    "\n",
    "$$AIC_C = AIC + \\frac{2(m+1)(m+2)}{n-m-2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other information criteria: \n",
    "- AICC,  AIC, SIC\n",
    "- FPE (Akaike, 1970, eqn (4.7)); FPE4 (Bhansali & Downham, 1977, p. 547)\n",
    "- HQ (Hannan & Quinn, 1979, p. 191)\n",
    "- CP (Mallows, 1973, eqn (3)); \n",
    "- PRESS (Allen, 1974, p. 126). \n",
    "\n",
    "Of these\n",
    "criteria, HQ and sic are consistent (Shibata, 1986), and AICC, AIC, FPE, CP are asymptotically efficient (Shibata, 1981, p. 53)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [A Comparative Analysis of Short Time Series Processing Methods, Kirshners (2012)](https://www.researchgate.net/publication/234838605_A_Comparative_Analysis_of_Short_Time_Series_Processing_Methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This article analyzes the traditional time series\n",
    "processing methods that are used to perform the task of short\n",
    "time series analysis in demand forecasting. The main aim of this\n",
    "paper is to scrutinize the ability of these methods to be used when\n",
    "analyzing short time series. The analyzed methods include\n",
    "exponential smoothing, exponential smoothing with the\n",
    "development trend and moving average method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The methods of the research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Moving Average Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving average method is used if the time series does not\n",
    "show a strong trend, and the demand data fluctuate around a\n",
    "certain mean value [4]. In this situation, the moving average\n",
    "method can be used for forecasting. The value that is used for\n",
    "forecasting is also called an adaptive average; the values are\n",
    "attributed to the end of the interval, compared to moving\n",
    "average in statistics where smoothing is directed towards the\n",
    "middle of the interval.\n",
    "\n",
    "$$M_t=\\frac{( x_t + x_{t-1} + ... + x_{t-N +1 })}{N}$$\n",
    "\n",
    "The limitations of the moving average method are related to\n",
    "the requirement for large amount of historic statistics because,\n",
    "when they are smoothed, the average data are summed,\n",
    "decreasing the overall number of periods.\n",
    "\n",
    "According to the conducted experiments, the moving\n",
    "average method  achieves the smallest squared\n",
    "error value, but the forecast smoothing regarding the initial\n",
    "historical data is too large. This indicates that forecasting\n",
    "results are too much smoothed and they do not reflect the\n",
    "actual demand. __Therefore,\n",
    "this method should not be used in short time series forecasting.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exponential Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exponential smoothing is based on historical demand data,\n",
    "forecast of the preceding period and the calculation of its\n",
    "squared error. This method uses weight correction that\n",
    "corrects the last demand in the historical time series.\n",
    "$$F_{t+1} = \\alpha R_t + (1-\\alpha) f_t$$\n",
    "where $F_{t+1}$ – the forecast for the next period; $0<\\alpha<1$ – the constant of exponential smoothing; $R_t$ – the actual demand of the previous period; $f_t$ – the forecast of the previous period.\n",
    "\n",
    "High values of $\\alpha$ are used to forecast demand with short historical\n",
    "information, but low values of $\\alpha$ are used to forecast large\n",
    "amount of statistical data. The correct choice of the constant\n",
    "value of exponential smoothing diminishes the squared error\n",
    "$$SE=\\sqrt{\\frac{\\sum(F_t-R_t)^2}{n-1}}$$\n",
    "\n",
    "The exponential smoothing has shown the largest squared\n",
    "error value using the chosen dataset; the forecasting result\n",
    "curve of this method repeats the fluctuation of historical data.\n",
    "It means that theoretically this method can be used for short\n",
    "time series forecasting because it reacts to both sharp and\n",
    "slight changes in demand.\n",
    "\n",
    "To decrease the value of\n",
    "squared error, values of the exponential smoothing $\\alpha$ can be\n",
    "used by assigning different weights to different dynamic levels\n",
    "of the series. For example, if it is known that the forecast is\n",
    "influenced by the closest levels of the previous series, then\n",
    "value of $\\alpha$ should be larger, but, if the main influence comes\n",
    "from the previous values, then it should be smaller.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exponential Smoothing with the Development Trend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method of exponential smoothing with the development\n",
    "trend is based on the sum of two equations that describe the\n",
    "short-time forecast  and trend. The short-time\n",
    "forecast is calculated as shown in the equation:\n",
    "$$p_t = \\alpha R_t + (1-\\alpha)( p_t + T_t )$$\n",
    "where $ T_{ t+1}$ - trend;  $p_t$ – the forecast for period $t$;\n",
    "$R_t$ – the actual demand of the previous period\n",
    "\n",
    "The trend is calculated according to the equation:\n",
    "$$T_{t +  1} = \\beta ( p_{t + 1}- p_t ) +  T_t  (1-\\beta )$$ \n",
    "\n",
    "The trend smoothing constant $\\beta$ is estimated, based on the\n",
    "assumption similar to exponential smoothing constant – the\n",
    "larger value is chosen to be $\\beta$, the larger the influence of the\n",
    "short-time forecast will be and the smaller the impact of the\n",
    "trend will be.\n",
    "\n",
    "The advantage of the exponential smoothing and\n",
    "exponential smoothing with the development trend methods is\n",
    "their ability to operatively calculate forecasts for the next\n",
    "period based on the previous period; but their disadvantage is\n",
    "the lack of information about forecasts after the calculated\n",
    "period because both methods are based on the actual value of\n",
    "the preceding periods and the comparison of their forecast\n",
    "error.\n",
    "\n",
    "__This method is also not recommended for\n",
    "use in short time series forecasting, because of too much smoothing.__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
