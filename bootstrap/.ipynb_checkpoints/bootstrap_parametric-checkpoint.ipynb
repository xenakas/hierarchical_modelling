{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Resampling Methods for Time Series](http://www-stat.wharton.upenn.edu/~stine/stat910/lectures/13_bootstrap.pdf)\n",
    "\n",
    "#### Parametric bootstrap\n",
    "\n",
    "Suppose we know that the underlying process is AR(1). Then we can estimate the parameters and general bootstrap data as \n",
    "$$X^*_t = \\hat{φ}X^*_{t−1 } + w^*_t$$ \n",
    "where $w^*_ t ∼ G_n$ and $G_n$ is the empirical distribution of the estimated model residuals. This works, but requires that we know the data generating process. \n",
    "\n",
    "The use of long autoregressions in estimation suggests a workable approach, knowns as a __sieve bootstrap__. \n",
    "\n",
    "Idea: Fit an AR model with a large number of lags and use that model to generate the bootstrap replications. The success clearly depends on how well the AR model captures the dependence and the ratio of the length $n$ to the number of AR coefficients $p$. \n",
    "\n",
    "Parameter bias: The estimates of the AR coefficients are biased. Hence the “true” model that generates the bootstrap series is shifted away from the actual generating process. The effects of the bias are most pronounced for coefficients of processes with zeros near the unit circle  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
