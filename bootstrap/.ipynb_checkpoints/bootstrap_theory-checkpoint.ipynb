{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The bootstrap technique introduced by Efron (1979) could possibly be a potential alternative in estimation and inference from time series models in finite samples. However, in time series regressions, the standard bootstrap resampling method designed for independent and identically distributed (IID) errors is not applicable because in most situations the assumption of IID errors is violated. \n",
    "\n",
    "The basic bootstrap approach consists of drawing repeated samples (with replacement). \n",
    "The simplest assumption for that method is that observations should be IID. \n",
    "But in time series models IID assumption is not satisfied.\n",
    "Thus the method needs to be modified.\n",
    "\n",
    "- Estimating Standard Errors:  if “Small Sample Size” distribution is normal then we can get a BS distribution to Estimate SE (same as asymptotic distribution for SE)\n",
    "- Confidence Interval statements:\n",
    "\tUsing BS distribution to Estimate CI we can get  different result for CI (from asymptotic distribution), for example, because of BS distribution skewness \n",
    "\n",
    "\n",
    "\n",
    "1. CI for $\\theta$ (Эфронов доверительный интервал)\n",
    "\n",
    "$$(q_{\\alpha/2}, q_{1-\\alpha/2})$$\n",
    "\n",
    "BS Distribution of $ \\hat{\\theta}^*$\n",
    "\n",
    "2. CI for assymptotic distribution of $\\hat{\\theta}$:\n",
    "$$(\\hat{\\theta}  \\pm z_{\\alpha} se(\\hat{\\theta}) )$$\n",
    "\n",
    "where $z_{\\alpha}$  -  the $100 - \\alpha$ percentile from the standart normal distribution\n",
    "\n",
    "3. CI for BS distribution of $\\hat{\\theta}$ (Доверительный интервал Холла):\n",
    "$$(\\hat{\\theta} -   z_{1-\\alpha}^*,  \\hat{\\theta} +   z_{\\alpha}^* ) $$\n",
    "\n",
    "\n",
    "where $z_{\\alpha}^*$  -  the $\\alpha$ percentile of the  distribution of $\\hat{\\theta}^* - \\hat{\\theta}$ (бутстрапируем  отклонение оценки от истинного значения).\n",
    "BS Distribution $ (\\hat{\\theta}^* - \\hat{\\theta} ) $, \n",
    "not $ (\\hat{\\theta}^* - \\theta_0 ) $\n",
    "\n",
    "4. t-percentile CI (t-процентильный доверительный интервал)\n",
    "\n",
    "$$(\\hat{\\theta} -   z_{1-\\alpha/2}^* se(\\hat{\\theta}),  \\hat{\\theta} +   z_{\\alpha/2}^* se(\\hat{\\theta})) $$\n",
    "\n",
    "BS properly studentized statistic:\n",
    "$$\\dfrac{\\hat{\\theta}^* - \\hat{\\theta}}{ \\hat{\\sigma}^*} $$\n",
    "use $\\hat{\\sigma}^*$ - estimate of $\\hat{\\sigma} $ from the BS sample\n",
    "\n",
    "Для получения симметричного t-процентильного CI (подходит для тестирования гипотез) \n",
    "$$(\\hat{\\theta} \\pm   z_{1-\\alpha}^* se(\\hat{\\theta})) $$\n",
    "  вместо $\\dfrac{\\hat{\\theta} - {\\theta}}{ \\hat{\\sigma}}$ бутстрапируем $\\left|\\dfrac{\\hat{\\theta} - {\\theta}}{ \\hat{\\sigma}}\\right|$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Recursive BS for stationary AR(p) model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Consider AR(p) process:\n",
    "$$ y_t = \\sum_{i=1}^p a_i y_{t-i} + e_t, e_t \\sim N(0,\\sigma^2)$$\n",
    "\n",
    "We estimate coefficients with OLS and get: \n",
    "$ (\\hat{a}_1,\\dots, \\hat{a}_p), \\hat{e}_t $\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " Define the centered and scaled residuals:\n",
    "$$ \\tilde{e}_t = (\\hat{e}_{t} - \\frac{1}{n} \\sum \\hat{e}_{t} )  \\left( \\frac{n}{n-p}\\right) ^{1/2} $$\n",
    "Resample $ \\tilde{e}_t $ with replacement to get the BS residuals $ e_t^* $\n",
    "\n",
    "Construct the BS sample recursively using $ y_t^* = y_t $:\n",
    "\n",
    "$$ y_t^* = \\sum_{i=1}^p \\hat{a}_i y_{t-i}^* + e_t^*$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Bootstrap\n",
    "\n",
    "###  [Bootstraping](https://quantdare.com/bootstrapping-time-series-data/)\n",
    "\n",
    "Bootstrapping is a well-known technique used to estimate the properties of an statistic. It was developed by Bradley Efron in 1979. The most common use cases include estimating variances and/or confidence intervals. Also, we have already seen how to apply it to portfolio management here on Quantdare. The technique is conceptually very simple: it relies on random sampling with replacement. The general idea is that by doing so we are effectively sampling from a distribution that matches the empirical distribution of the current sample, which can be seen as an approximation of sampling from the actual population distribution. It is a very simple and powerful approach that allows approximating the solution of problems that would be, otherwise, impossible or very tedious to solve.\n",
    "\n",
    "\n",
    "And that is exactly what we are seeing; by sampling randomly without constraints, we are destroying the time-dependence structure in the time series. This is the main limitation of the tradicional bootstrapping method and, to make it explicit, it is sometimes referred to as independent and identically distributed (IID) bootstrap.\n",
    "\n",
    "### [ Bootstrapping time series – R code](https://eranraviv.com/bootstrapping-time-series-r-code/)\n",
    "\n",
    "1.  Bootstrap based on IID innovations\n",
    "\n",
    "The idea is to estimate the model, and then use the residuals that are, by construction, close to being independent. Bootstrap these residuals and “back out” the observations using your estimated parameters.\n",
    "\n",
    "2. Block Bootstrap (or MBB for moving block bootstrapping)\n",
    "\n",
    "Essentially, we cannot sample the data directly because we lose the dependency structure. Solution is to sample whole blocks and concatenate them, in contrast to a single observation at a time.\n",
    "\n",
    "\n",
    "\n",
    "[Numerous approaches for handling dependent data](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.182.1011&rep=rep1&type=pdf):\n",
    "\n",
    "For $X_1, \\dots , X_n \\sim i.i.d.$, the\n",
    "IID Bootstrap approximation for $Pr \\{ T_n \\leq x \\}$ is far more accurate\n",
    "than the classical normal approximation.\n",
    "But, in the face of dependent data\n",
    "$$ \\lim_{n \\to \\infty} [ Pr_{*}(T^{∗}_n \\leq x) − Pr(T_n \\leq x)] \\neq 0\n",
    "$$ \n",
    "\n",
    "- Parametric Bootstrap (Efron and Tibshirani, 1998, pp. 53–55)\n",
    "    -  Bootstrap based on IID Innovations (Lahiri, 2003, pp. 23-24)\n",
    "- Block Bootstrap methods (Wilks, 1997)\n",
    "- Subsampling (Carlstein, 1986)\n",
    "- Transformation-based (TBB) (Hurvich and Zeger, 1987)\n",
    "- Sieve Bootstrap (Lahiri, 2003, pp. 41–43)\n",
    "\n",
    "#### [ The IID Bootstrap method](http://www.math.chalmers.se/~palbin/BootstrapDependentAndreasSunesson.pdf)\n",
    "\n",
    "The bootstrap method is a commonly used way of checking the distribution function of some\n",
    "estimator on a time series. Here we give a definition of the method and supply an exampel, with\n",
    "dependent data, where the standard method fails. We then continue to expand the method to\n",
    "behave descent for dependent data.\n",
    "\n",
    "\n",
    "\n",
    "The main principle of the bootstrap method is resampling from a known data set, to give a distribution of the estimator $\\hat{\\theta}$. This is done in the IID Bootstrap–method by picking\n",
    "n numbers from ${1, \\dots, n}$ with equal probability and with replacement. Call a number chosen\n",
    "this way $U$ to get a sequence $U_i$. \n",
    "Then construct a resample by chosing n data points\n",
    "$X_{U_i}$, call this series $\\tilde{X}_1, \\dots , \\tilde{X}_n$ and then applying this resample to the estimator $\\hat{\\theta}$. By\n",
    "repeating this process N times we get a series of estimations \n",
    "$\\{ \\hat{\\theta} (\\tilde{X}_1^j, \\dots , \\tilde{X}_n^j) \\}^N_{j=1}$\n",
    "which can be\n",
    "used to construct a distribution function, $P(\\hat{\\theta} \\leq x) $\n",
    "of $\\hat{\\theta}$.\n",
    "\n",
    "\n",
    "This however requires the data to be independent.\n",
    "If the data is in fact dependent, the method fails to give a proper estimate, as seen in\n",
    "following example, modified from an article by Eola Investments , LLC \n",
    "\n",
    "*Example of IID Bootstrap shortcomings*\n",
    "\n",
    "The article proposes a gamble, you’re allowed to buy as large a series as you want for $1 per number.\n",
    "The series you get is comprised of consecutive 0s and 1s. For every sequence of ”10101” you\n",
    "can show in that series, you win 1000.\n",
    "\n",
    "To analyze this gamble, a series of 10000 numbers is aquired\n",
    "$$ X = {1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, . . . }.$$ \n",
    "The mean of this series\n",
    "lies at 0.55 which implies that the probability, $p$, of encountering a 1, looking anywhere in the\n",
    "series, is 0.55. \n",
    "\n",
    "Assuming that the data is independent of each other, the expected value of ”10101’s”\n",
    "in a 10000 number long series is thus roughly \n",
    "$$ \\approx p *(1 − p)* p *(1 − p)* p * 10000 \\approx 0.0337  * 10000 $$ \n",
    "\n",
    "Applying the\n",
    "IID Bootstrap–method to this data to estimate the possibility of having an occurence of ”10101”\n",
    "gives a 95\\% confidence interval of $[295.42, 335.23, 375.03]$. \n",
    "However, going back to the original data\n",
    "and actually counting the number of occurences results in a finding of only 5.\n",
    "\n",
    "\n",
    "This huge discrepancy found is explained from the fact that when calculating the theoretical expected\n",
    "value of occurences, we assumed that the sequence was IID, obviously, this was also the case\n",
    "with the IID Bootstrap–method. However, the sequence was constructed by a simple algorithm\n",
    "that took the previous value and kept it with probability 0.7, and changed the value to either 0 or\n",
    "1, both with equal probability, with probability 0.3. Clearly, this sequence is not independent, and\n",
    "Monte–Carlo simulations from this process give the expected number of occurences of ”10101’s” to\n",
    "be 5.89.\n",
    "\n",
    "#### Determining sample size necessary for bootstrap method\n",
    "\n",
    "Now if the sample size is very small---say 4---the bootstrap may not work just because the set of possible bootstrap samples is not rich enough. In my book or Peter Hall's book this issue of too small a sample size is discussed. But this number of distinct bootstrap samples gets large very quickly. So this is not an issue even for sample sizes as small as 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_(statistics )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In statistics, bootstrapping is any test or metric that relies on random sampling with replacement. This technique allows estimation of the sampling distribution of almost any statistic using random sampling methods. Generally, it falls in the broader class of resampling methods.\n",
    "\n",
    "Bootstrapping is the practice of estimating properties of an estimator (such as its variance) by measuring those properties when sampling from an approximating distribution. One standard choice for an approximating distribution is the empirical distribution function of the observed data. In the case where a set of observations can be assumed to be from an independent and identically distributed population, this can be implemented by constructing a number of resamples with replacement, of the observed dataset (and of equal size to the observed dataset).\n",
    "\n",
    "It may also be used for constructing hypothesis tests. It is often used as an alternative to statistical inference based on the assumption of a parametric model when that assumption is in doubt, or where parametric inference is impossible or requires complicated formulas for the calculation of standard errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bootstrap was published by Bradley Efron in \"Bootstrap methods: another look at the jackknife\" (1979), inspired by earlier work on the jackknife. Improved estimates of the variance were developed later. A Bayesian extension was developed in 1981. The bias-corrected and accelerated (BCa) bootstrap was developed by Efron in 1987, and the ABC procedure in 1992."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Resampling Methods for Time Series](http://www-stat.wharton.upenn.edu/~stine/stat910/lectures/13_bootstrap.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main idea: estimate the sampling distribution of a statistic, with particular emphasis on the standard error of the statistic and finding a confidence interval for a statistic.\n",
    "The sampling distribution of a statistic $\\hat{θ}$ is the distribution  computed from repeated samples $X$ of size $n$ from some distribution $F$. The trick is to sample from the data itself rather than the\n",
    "population.\n",
    "\n",
    "Bootstrap sample is a sample drawn with replacement from the original\n",
    "sample $X$, denoted by $X^∗ ∼ F_n$ where $F_n$ is the empirical distribution\n",
    "of the observed sample $X$\n",
    "\n",
    "__Bootstrap\n",
    "resampling is not about simulation, it’s about using the empirical distribution $F_n$ in place of $F$. We simulate because its easy and because\n",
    "we often need this for statistics that are not linear.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider two linear expansions, thinking of $T$ as an operator on distributions and $T'$ as a derivative (Frechet), \n",
    "$$\\hat{θ}^* = T(F^*_n ) ≈ T(F_n) + T'(F_n)(F^*_n − F_n) \\\\  \\hat{θ} = T(F_n) ≈ T(F) + T' (F)(F_n − F)$$  \n",
    "You get a hint that things are going in a nice direction because both $F_n − F$ and $F^*_n − F_n$ tend to a Brownian bridge as $n \\to \\infty$. Of course, we also need for the derivatives to be close as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  [Эконометрический ликбез: бутстрап](http://quantile.ru/03/03-SA.pdf)\n",
    "\n",
    "- Бутстрапирование условных моделей временных рядов включает в себя [марковский бутстрап](https://www.ssc.wisc.edu/~bhansen/718/Horowitz%20Markov%20Bootstrap.pdf) (Horowitz, 2003) и его упрощенную версию – бутстрапирование марковской цепью (Anatolyev & Vasnev, 2002).\n",
    "- [Бутстрапирование моделей VAR](https://mpra.ub.uni-muenchen.de/23503/1/MPRA_paper_23503.pdf) и [функций импульсного отклика](https://www.ssc.wisc.edu/~bhansen/718/Kilian1998.pdf) обсуждается в Kilian (1998) и Kilian (1999), а [применение бутстрапа при прогнозировании](https://www.sciencedirect.com/science/article/pii/S0169207099000060) – в Kim (1999).\n",
    "- [Бутстрапирование моделей с единичным корнем](http://fmwww.bc.edu/RePEc/es2000/0401.pdf) и [соответствующих тестов](https://onlinelibrary.wiley.com/doi/abs/10.1111/1468-0262.00471) обсужда- ется в Inoue & Kilian (2002) и Park (2003), а [сеточный бутстрап для моделей с корнем, близким к единичному](https://www.ssc.wisc.edu/~bhansen/progs/restat_99.html) – в Hansen (1999)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
