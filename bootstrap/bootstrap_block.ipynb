{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Moving Block Bootstrap\n",
    "\n",
    "Application of the residual based bootstrap methods is straightforward if the error distribution is specified to be an ARMA(p,q) process with known p and q\n",
    "\n",
    "However, if the structure of serial |correlation is not tractable or is misspecified, the residual based \n",
    "methods will give inconsistent estimates\n",
    "\n",
    "Divide the data of $n$ observations into blocks of length $l$ and select $b$ of these blocks (with repeats allowed) \n",
    "\n",
    "\n",
    "** NBB - nonoverlapping blocks bootstrap **\n",
    "\n",
    "> Carlstein (1986) – first discussed the idea of bootstrapping blocks of observations rather \n",
    "than the individual observations.\n",
    "\n",
    "Number of blocks: $\\frac{n}{l} = b$  \n",
    "\n",
    "High probability of missing entire blocks in the Carlstein scheme (non overlapping blocks) $ \\rightarrow $ not often used\n",
    "\n",
    "** MBB - moving blocks bootstrap **\n",
    "\n",
    "\n",
    "> Künsch (1989) and Singh (1992) – independently introduced a more general BS\n",
    "procedure, the moving block BS (MBB) which is applicable to stationary time series data. In this method the blocks of observations are overlapping.\n",
    "\n",
    "Number of blocks: $n - l + 1$  \n",
    "\n",
    ">> IDEA: MBB for short clusterized time series\n",
    "\n",
    "#### Problems with MBB\n",
    "\n",
    "1. The pseudo time series generated by the moving block method is not stationary, even if the original series $\\{x_t\\}$ is stationary\n",
    "\n",
    "\n",
    "> Politis and Romano (1994):  **A stationary bootstrap method**\n",
    "\n",
    " Sampling blocks of random length, where the length of each block has a geometric distribution. They show that the pseudo time series generated by the stationary bootstrap method is indeed stationary\n",
    "\n",
    "The application of stationary bootstrap is less sensitive to the choice of $p$ than the application of moving block bootstrap is to the choice of $l$\n",
    "\n",
    "2. The mean $\\bar{x}^*_n$ of the moving block bootstrap is biased in the sense that: $$E(\\bar{x}^*_n | x_1, ... , x_n) \\neq \\bar{x}_n $$\n",
    "\n",
    "\n",
    "3. The MBB estimator of the variance of $\\sqrt{n} \\bar{x}_n$  is also biased\n",
    "\n",
    "> Davidson and Hall (1993): ** modification **\n",
    "\n",
    " Usual estimator: $\\hat{\\sigma}^2 = n^{-1}\\sum^n_{i=1}(x_i - \\bar{x}_n )^2$\n",
    "\n",
    " Modification:  $\\tilde{\\sigma}^2 = n^{-1}\\sum^n_{i=1}\\left((x_i - \\bar{x}_n )^2  + \\sum^{i-1}_{k=1} \\sum^{n-k}_{i=1} (x_i - \\bar{x}_n ) (x_{i+k} - \\bar{x}_n ) \\right)$\n",
    " \n",
    " With this modification the bootstrap can improve substantially on the normal approximation\n",
    "\n",
    "#### Optimal Length of Blocks\n",
    "\n",
    "Interested in minimizing the MSE of the block bootstrap estimate\n",
    "of the variance of a general statistic\n",
    "\n",
    "Carlstein’s rules for non-overlapping blocks: \n",
    "\n",
    "- As the block size increases: variance $\\uparrow$,  bias  $\\downarrow$\n",
    "\n",
    "- As the dependency among the $x_i$ gets stronger  a longer block size is needed\n",
    "\n",
    "- Optimal block size for AR(1) model $x_t = \\phi x_{t-1} + e_t $  is $l^* = \\left( \\dfrac{2\\phi}{1-\\phi^2}  \\right)^{2/3} n^{2/3}$ \n",
    "\n",
    "- Carlstein optimal block size:  $ l = n^{1/3} \\rho^{-2/3}$\n",
    "\n",
    "- Künsch optimal block size: $ l = (3/2 * n)^{1/3} \\rho^{-2/3} $, where the covariance of $x_t$ at lag $j$:\n",
    " \n",
    "$$ \\rho  = \\dfrac{\\gamma(0) + 2 \\sum^{\\infty}_{j=1} \\gamma(j) }{ \\sum^{\\infty}_{j=1} j \\gamma(j)}$$ \n",
    "\n",
    "- Hall and Horowitz’s rules for AR(1): $ \\rho = (1-\\phi^2)/\\phi$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Resampling Methods for Time Series](http://www-stat.wharton.upenn.edu/~stine/stat910/lectures/13_bootstrap.pdf)\n",
    "\n",
    "####  Subsampling\n",
    "\n",
    "Subsampling relies on the same ideas that we used in proving the CLT for a stationary process: arrange the data into blocks, and rely on the blocks becoming less dependent as they get farther apart. The key assumption is that the distribution of the statistic has the form \n",
    "$$\\sqrt{ n}( \\hat{θ} − θ) ∼ G$$\n",
    "Procedure \n",
    "1. Arrange the time series $X_1,  . . . , X_n$ into $N$ overlapping blocks, each of length $b$, with overlap\n",
    "2. Treat each of the $N$ blocks as if it were the time series of interest, computing the relevant statistic, obtaining $\\tilde{θ}_1,  . . . , \\tilde{θ}_N$.  \n",
    "3. Estimate the sampling properties of the estimator computed from the original time series from the rescaled empirical distribution of the $\\tilde{\\theta}_n$, \n",
    "$$\\tilde{F}_b(x) = 1/N \\sum_i H_{\\sqrt{ b}( \\hat{θ}_i − \\hat{θ})} (x)$$\n",
    "\n",
    "For iid samples, the theory resembles that used when studying leave-out-several versions of the jackknife and cross-validation. \n",
    "Subsampling provably works in many applications with the type of “weak” dependence associated with ARMA processes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
