{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Resampling Methods for Time Series](http://www-stat.wharton.upenn.edu/~stine/stat910/lectures/13_bootstrap.pdf)\n",
    "\n",
    "####  Subsampling\n",
    "\n",
    "Subsampling relies on the same ideas that we used in proving the CLT for a stationary process: arrange the data into blocks, and rely on the blocks becoming less dependent as they get farther apart. The key assumption is that the distribution of the statistic has the form \n",
    "$$\\sqrt{ n}( \\hat{θ} − θ) ∼ G$$\n",
    "Procedure \n",
    "1. Arrange the time series $X_1,  . . . , X_n$ into $N$ overlapping blocks, each of length $b$, with overlap\n",
    "2. Treat each of the $N$ blocks as if it were the time series of interest, computing the relevant statistic, obtaining $\\tilde{θ}_1,  . . . , \\tilde{θ}_N$.  \n",
    "3. Estimate the sampling properties of the estimator computed from the original time series from the rescaled empirical distribution of the $\\tilde{\\theta}_n$, \n",
    "$$\\tilde{F}_b(x) = 1/N \\sum_i H_{\\sqrt{ b}( \\hat{θ}_i − \\hat{θ})} (x)$$\n",
    "\n",
    "For iid samples, the theory resembles that used when studying leave-out-several versions of the jackknife and cross-validation. \n",
    "Subsampling provably works in many applications with the type of “weak” dependence associated with ARMA processes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
