{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The bootstrap technique introduced by Efron (1979) could possibly be a potential alternative in estimation and inference from time series models in finite samples. However, in time series regressions, the standard bootstrap resampling method designed for independent and identically distributed (IID) errors is not applicable because in most situations the assumption of IID errors is violated. \n",
    "\n",
    "The basic bootstrap approach consists of drawing repeated samples (with replacement). \n",
    "The simplest assumption for that method is that observations should be IID. \n",
    "But in time series models IID assumption is not satisfied.\n",
    "Thus the method needs to be modified.\n",
    "\n",
    "- Estimating Standard Errors:  if “Small Sample Size” distribution is normal then we can get a BS distribution to Estimate SE (same as asymptotic distribution for SE)\n",
    "- Confidence Interval statements:\n",
    "\tUsing BS distribution to Estimate CI we can get  different result for CI (from asymptotic distribution), for example, because of BS distribution skewness \n",
    "\n",
    "\n",
    "\n",
    "1. CI for $\\theta$ (Эфронов доверительный интервал)\n",
    "\n",
    "$$(q_{\\alpha/2}, q_{1-\\alpha/2})$$\n",
    "\n",
    "BS Distribution of $ \\hat{\\theta}^*$\n",
    "\n",
    "2. CI for assymptotic distribution of $\\hat{\\theta}$:\n",
    "$$(\\hat{\\theta}  \\pm z_{\\alpha} se(\\hat{\\theta}) )$$\n",
    "\n",
    "where $z_{\\alpha}$  -  the $100 - \\alpha$ percentile from the standart normal distribution\n",
    "\n",
    "3. CI for BS distribution of $\\hat{\\theta}$ (Доверительный интервал Холла):\n",
    "$$(\\hat{\\theta} -   z_{1-\\alpha}^*,  \\hat{\\theta} +   z_{\\alpha}^* ) $$\n",
    "\n",
    "\n",
    "where $z_{\\alpha}^*$  -  the $\\alpha$ percentile of the  distribution of $\\hat{\\theta}^* - \\hat{\\theta}$ (бутстрапируем  отклонение оценки от истинного значения).\n",
    "BS Distribution $ (\\hat{\\theta}^* - \\hat{\\theta} ) $, \n",
    "not $ (\\hat{\\theta}^* - \\theta_0 ) $\n",
    "\n",
    "4. t-percentile CI (t-процентильный доверительный интервал)\n",
    "\n",
    "$$(\\hat{\\theta} -   z_{1-\\alpha/2}^* se(\\hat{\\theta}),  \\hat{\\theta} +   z_{\\alpha/2}^* se(\\hat{\\theta})) $$\n",
    "\n",
    "BS properly studentized statistic:\n",
    "$$\\dfrac{\\hat{\\theta}^* - \\hat{\\theta}}{ \\hat{\\sigma}^*} $$\n",
    "use $\\hat{\\sigma}^*$ - estimate of $\\hat{\\sigma} $ from the BS sample\n",
    "\n",
    "Для получения симметричного t-процентильного CI (подходит для тестирования гипотез) \n",
    "$$(\\hat{\\theta} \\pm   z_{1-\\alpha}^* se(\\hat{\\theta})) $$\n",
    "  вместо $\\dfrac{\\hat{\\theta} - {\\theta}}{ \\hat{\\sigma}}$ бутстрапируем $\\left|\\dfrac{\\hat{\\theta} - {\\theta}}{ \\hat{\\sigma}}\\right|$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Recursive BS for stationary AR(p) model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Consider AR(p) process:\n",
    "$$ y_t = \\sum_{i=1}^p a_i y_{t-i} + e_t, e_t \\sim N(0,\\sigma^2)$$\n",
    "\n",
    "We estimate coefficients with OLS and get: \n",
    "$ (\\hat{a}_1,\\dots, \\hat{a}_p), \\hat{e}_t $\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " Define the centered and scaled residuals:\n",
    "$$ \\tilde{e}_t = (\\hat{e}_{t} - \\frac{1}{n} \\sum \\hat{e}_{t} )  \\left( \\frac{n}{n-p}\\right) ^{1/2} $$\n",
    "Resample $ \\tilde{e}_t $ with replacement to get the BS residuals $ e_t^* $\n",
    "\n",
    "Construct the BS sample recursively using $ y_t^* = y_t $:\n",
    "\n",
    "$$ y_t^* = \\sum_{i=1}^p \\hat{a}_i y_{t-i}^* + e_t^*$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Moving Block Bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application of the residual based bootstrap methods is straightforward if the error distribution is specified to be an ARMA(p,q) process with known p and q\n",
    "\n",
    "However, if the structure of serial correlation is not tractable or is misspecified, the residual based methods will give inconsistent estimates\n",
    "\n",
    "Divide the data of $n$ observations into blocks of length $l$ and select $b$ of these blocks (with repeats allowed) \n",
    "\n",
    "\n",
    "** NBB - nonoverlapping blocks bootstrap **\n",
    "\n",
    "> Carlstein (1986) – first discussed the idea of bootstrapping blocks of observations rather \n",
    "than the individual observations.\n",
    "\n",
    "Number of blocks: $\\frac{n}{l} = b$  \n",
    "\n",
    "High probability of missing entire blocks in the Carlstein scheme (non overlapping blocks) $ \\rightarrow $ not often used\n",
    "\n",
    "** MBB - moving blocks bootstrap **\n",
    "\n",
    "\n",
    "> Künsch (1989) and Singh (1992) – independently introduced a more general BS\n",
    "procedure, the moving block BS (MBB) which is applicable to stationary time series data. In this method the blocks of observations are overlapping.\n",
    "\n",
    "Number of blocks: $n - l + 1$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> IDEA: MBB for short clusterized time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problems with MBB\n",
    "\n",
    "1. The pseudo time series generated by the moving block method is not stationary, even if the original series $\\{x_t\\}$ is stationary\n",
    "\n",
    "\n",
    "> Politis and Romano (1994):  **A stationary bootstrap method**\n",
    "\n",
    " Sampling blocks of random length, where the length of each block has a geometric distribution. They show that the pseudo time series generated by the stationary bootstrap method is indeed stationary\n",
    "\n",
    "The application of stationary bootstrap is less sensitive to the choice of $p$ than the application of moving block bootstrap is to the choice of $l$\n",
    "\n",
    "2. The mean $\\bar{x}^*_n$ of the moving block bootstrap is biased in the sense that: $$E(\\bar{x}^*_n | x_1, ... , x_n) \\neq \\bar{x}_n $$\n",
    "\n",
    "\n",
    "3. The MBB estimator of the variance of $\\sqrt{n} \\bar{x}_n$  is also biased\n",
    "\n",
    "> Davidson and Hall (1993): ** modification **\n",
    "\n",
    " Usual estimator: $\\hat{\\sigma}^2 = n^{-1}\\sum^n_{i=1}(x_i - \\bar{x}_n )^2$\n",
    "\n",
    " Modification:  $\\tilde{\\sigma}^2 = n^{-1}\\sum^n_{i=1}\\left((x_i - \\bar{x}_n )^2  + \\sum^{i-1}_{k=1} \\sum^{n-k}_{i=1} (x_i - \\bar{x}_n ) (x_{i+k} - \\bar{x}_n ) \\right)$\n",
    " \n",
    " With this modification the bootstrap can improve substantially on the normal approximation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimal Length of Blocks\n",
    "\n",
    "Interested in minimizing the MSE of the block bootstrap estimate\n",
    "of the variance of a general statistic\n",
    "\n",
    "Carlstein’s rules for non-overlapping blocks: \n",
    "\n",
    "- As the block size increases: variance $\\uparrow$,  bias  $\\downarrow$\n",
    "\n",
    "- As the dependency among the $x_i$ gets stronger  a longer block size is needed\n",
    "\n",
    "- Optimal block size for AR(1) model $x_t = \\phi x_{t-1} + e_t $  is $l^* = \\left( \\dfrac{2\\phi}{1-\\phi^2}  \\right)^{2/3} n^{2/3}$ \n",
    "\n",
    "- Carlstein optimal block size:  $ l = n^{1/3} \\rho^{-2/3}$\n",
    "\n",
    "- Künsch optimal block size: $ l = (3/2 * n)^{1/3} \\rho^{-2/3} $, where the covariance of $x_t$ at lag $j$:\n",
    " \n",
    "$$ \\rho  = \\dfrac{\\gamma(0) + 2 \\sum^{\\infty}_{j=1} \\gamma(j) }{ \\sum^{\\infty}_{j=1} j \\gamma(j)}$$ \n",
    "\n",
    "- Hall and Horowitz’s rules for AR(1): $ \\rho = (1-\\phi^2)/\\phi$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Bootstrap\n",
    "\n",
    "###  [Bootstraping](https://quantdare.com/bootstrapping-time-series-data/)\n",
    "\n",
    "Bootstrapping is a well-known technique used to estimate the properties of an statistic. It was developed by Bradley Efron in 1979. The most common use cases include estimating variances and/or confidence intervals. Also, we have already seen how to apply it to portfolio management here on Quantdare. The technique is conceptually very simple: it relies on random sampling with replacement. The general idea is that by doing so we are effectively sampling from a distribution that matches the empirical distribution of the current sample, which can be seen as an approximation of sampling from the actual population distribution. It is a very simple and powerful approach that allows approximating the solution of problems that would be, otherwise, impossible or very tedious to solve.\n",
    "\n",
    "\n",
    "And that is exactly what we are seeing; by sampling randomly without constraints, we are destroying the time-dependence structure in the time series. This is the main limitation of the tradicional bootstrapping method and, to make it explicit, it is sometimes referred to as independent and identically distributed (IID) bootstrap.\n",
    "\n",
    "### [ Bootstrapping time series – R code](https://eranraviv.com/bootstrapping-time-series-r-code/)\n",
    "\n",
    "1.  Bootstrap based on IID innovations\n",
    "\n",
    "The idea is to estimate the model, and then use the residuals that are, by construction, close to being independent. Bootstrap these residuals and “back out” the observations using your estimated parameters.\n",
    "\n",
    "2. Block Bootstrap (or MBB for moving block bootstrapping)\n",
    "\n",
    "Essentially, we cannot sample the data directly because we lose the dependency structure. Solution is to sample whole blocks and concatenate them, in contrast to a single observation at a time.\n",
    "\n",
    "\n",
    "\n",
    "[Numerous approaches for handling dependent data](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.182.1011&rep=rep1&type=pdf):\n",
    "\n",
    "For $X_1, \\dots , X_n \\sim i.i.d.$, the\n",
    "IID Bootstrap approximation for $Pr \\{ T_n \\leq x \\}$ is far more accurate\n",
    "than the classical normal approximation.\n",
    "But, in the face of dependent data\n",
    "$$ \\lim_{n \\to \\infty} [ Pr_{*}(T^{∗}_n \\leq x) − Pr(T_n \\leq x)] \\neq 0\n",
    "$$ \n",
    "\n",
    "- Parametric Bootstrap (Efron and Tibshirani, 1998, pp. 53–55)\n",
    "    -  Bootstrap based on IID Innovations (Lahiri, 2003, pp. 23-24)\n",
    "- Block Bootstrap methods (Wilks, 1997)\n",
    "- Subsampling (Carlstein, 1986)\n",
    "- Transformation-based (TBB) (Hurvich and Zeger, 1987)\n",
    "- Sieve Bootstrap (Lahiri, 2003, pp. 41–43)\n",
    "\n",
    "#### [ The IID Bootstrap method](http://www.math.chalmers.se/~palbin/BootstrapDependentAndreasSunesson.pdf)\n",
    "\n",
    "The bootstrap method is a commonly used way of checking the distribution function of some\n",
    "estimator on a time series. Here we give a definition of the method and supply an exampel, with\n",
    "dependent data, where the standard method fails. We then continue to expand the method to\n",
    "behave descent for dependent data.\n",
    "\n",
    "\n",
    "\n",
    "The main principle of the bootstrap method is resampling from a known data set, to give a distribution of the estimator $\\hat{\\theta}$. This is done in the IID Bootstrap–method by picking\n",
    "n numbers from ${1, \\dots, n}$ with equal probability and with replacement. Call a number chosen\n",
    "this way $U$ to get a sequence $U_i$. \n",
    "Then construct a resample by chosing n data points\n",
    "$X_{U_i}$, call this series $\\tilde{X}_1, \\dots , \\tilde{X}_n$ and then applying this resample to the estimator $\\hat{\\theta}$. By\n",
    "repeating this process N times we get a series of estimations \n",
    "$\\{ \\hat{\\theta} (\\tilde{X}_1^j, \\dots , \\tilde{X}_n^j) \\}^N_{j=1}$\n",
    "which can be\n",
    "used to construct a distribution function, $P(\\hat{\\theta} \\leq x) $\n",
    "of $\\hat{\\theta}$.\n",
    "\n",
    "\n",
    "This however requires the data to be independent.\n",
    "If the data is in fact dependent, the method fails to give a proper estimate, as seen in\n",
    "following example, modified from an article by Eola Investments , LLC \n",
    "\n",
    "*Example of IID Bootstrap shortcomings*\n",
    "\n",
    "The article proposes a gamble, you’re allowed to buy as large a series as you want for $1 per number.\n",
    "The series you get is comprised of consecutive 0s and 1s. For every sequence of ”10101” you\n",
    "can show in that series, you win 1000.\n",
    "\n",
    "To analyze this gamble, a series of 10000 numbers is aquired\n",
    "$$ X = {1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, . . . }.$$ \n",
    "The mean of this series\n",
    "lies at 0.55 which implies that the probability, $p$, of encountering a 1, looking anywhere in the\n",
    "series, is 0.55. \n",
    "\n",
    "Assuming that the data is independent of each other, the expected value of ”10101’s”\n",
    "in a 10000 number long series is thus roughly \n",
    "$$ \\approx p *(1 − p)* p *(1 − p)* p * 10000 \\approx 0.0337  * 10000 $$ \n",
    "\n",
    "Applying the\n",
    "IID Bootstrap–method to this data to estimate the possibility of having an occurence of ”10101”\n",
    "gives a 95\\% confidence interval of $[295.42, 335.23, 375.03]$. \n",
    "However, going back to the original data\n",
    "and actually counting the number of occurences results in a finding of only 5.\n",
    "\n",
    "\n",
    "This huge discrepancy found is explained from the fact that when calculating the theoretical expected\n",
    "value of occurences, we assumed that the sequence was IID, obviously, this was also the case\n",
    "with the IID Bootstrap–method. However, the sequence was constructed by a simple algorithm\n",
    "that took the previous value and kept it with probability 0.7, and changed the value to either 0 or\n",
    "1, both with equal probability, with probability 0.3. Clearly, this sequence is not independent, and\n",
    "Monte–Carlo simulations from this process give the expected number of occurences of ”10101’s” to\n",
    "be 5.89.\n",
    "\n",
    "### [Parametric bootstrap](https://math.mit.edu/~dav/05.dir/class25-slides-all.pdf)\n",
    "\n",
    "Use the estimated parameter to estimate the variation of estimates of\n",
    "the parameter\n",
    "\n",
    "Data: $x_1, \\dots , x_n$ drawn from a parametric distribution $F(\\theta)$.\n",
    "\n",
    "Estimate $\\theta$ by a statistic $\\hat{\\theta}$ .\n",
    "\n",
    "Generate many bootstrap samples from $F(\\hat{\\theta})$.\n",
    "\n",
    "Compute the statistic $\\theta^{*}$ for each bootstrap sample.\n",
    "\n",
    "Compute the bootstrap difference\n",
    "$$\\delta^{*} = \\theta^{*} -  \\hat{\\theta}$$\n",
    "\n",
    "Use the quantiles of $\\delta^{*} $  to approximate quantiles of $\\delta =  \\hat{\\theta} - \\theta $\n",
    "\n",
    "Set a confidence interval\n",
    "$$ [  \\hat{\\theta}  − \\delta^{*}_{1 - \\alpha/2}; \\hat{\\theta}  − \\delta^{*}_{\\alpha/2}  ] $$ \n",
    "\n",
    "\n",
    "#### [Parametric bootstrap for TS](http://www-stat.wharton.upenn.edu/~stine/stat910/lectures/13_bootstrap.pdf)\n",
    "\n",
    "Suppose we know that the underlying process is AR(1). \n",
    "Then we can estimate the parameters and general bootstrap\n",
    "data as\n",
    "\n",
    "$$X^{∗}_t = \\hat{\\phi} Xˆ{∗}_{t−1} + w^{∗}_t$$\n",
    "\n",
    "where $w^{∗}_t \\sim G_n$ and $G_n$ is the empirical distribution of the estimated\n",
    "model residuals. This works, but requires that we know the data\n",
    "generating process.\n",
    "\n",
    "The use of long autoregressions in estimation suggests a workable approach,\n",
    "knowns as a sieve bootstrap. The idea is simple. \n",
    "Fit an AR\n",
    "model with a large number of lags and use that model to generate the\n",
    "bootstrap replications. \n",
    "The success clearly depends on how well the\n",
    "AR model captures the dependence and the ratio of \n",
    "the length n to\n",
    "the number of AR coefficients p.\n",
    "\n",
    "\n",
    "The estimates of the AR coefficients are biased. Hence\n",
    "the “true” model that generates the bootstrap series is shifted away\n",
    "from the actual generating process. \n",
    "\n",
    "\n",
    "#### Determining sample size necessary for bootstrap method\n",
    "\n",
    "Now if the sample size is very small---say 4---the bootstrap may not work just because the set of possible bootstrap samples is not rich enough. In my book or Peter Hall's book this issue of too small a sample size is discussed. But this number of distinct bootstrap samples gets large very quickly. So this is not an issue even for sample sizes as small as 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
